{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6oqGiIXvrMl"
      },
      "source": [
        "# CS224N: PyTorch Tutorial (Winter '21)\n",
        "\n",
        "### Author: Dilara Soylu\n",
        "\n",
        "In this notebook, we will have a basic introduction to `PyTorch` and work on a toy NLP task. Following resources have been used in preparation of this notebook:\n",
        "* [\"Word Window Classification\" tutorial notebook]((https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/materials/ww_classifier.ipynb) by Matt Lamm, from Winter 2020 offering of CS224N\n",
        "* Official PyTorch Documentation on [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) by Soumith Chintala\n",
        "* PyTorch Tutorial Notebook, [Build Basic Generative Adversarial Networks (GANs) | Coursera](https://www.coursera.org/learn/build-basic-generative-adversarial-networks-gans) by Sharon Zhou, offered on Coursera\n",
        "\n",
        "Many thanks to Angelica Sun and John Hewitt for their feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gk1UKaNvrMv"
      },
      "source": [
        "## Introduction\n",
        "[PyTorch](https://pytorch.org/) is a machine learning framework that is used in both academia and industry for various applications. PyTorch started of as a more flexible alternative to [TensorFlow](https://www.tensorflow.org/), which is another popular machine learning framework. At the time of its release, `PyTorch` appealed to the users due to its user friendly nature: as opposed to defining static graphs before performing an operation as in `TensorFlow`, `PyTorch` allowed users to define their operations as they go, which is also the approached integrated by `TensorFlow` in its following releases. Although `TensorFlow` is more widely preferred in the industry, `PyTorch` is often times the preferred machine learning framework for researchers. If you would like to learn more about the differences between the two, you can check out [this](https://blog.udacity.com/2020/05/pytorch-vs-tensorflow-what-you-need-to-know.html) blog post.\n",
        "\n",
        "PyTorch 是一个用于各种应用的机器学习框架，在学术界和工业界都有广泛的使用。PyTorch 最初作为 TensorFlow 的一个更灵活的替代品发布，后者是另一个流行的机器学习框架。在发布时，PyTorch 因其用户友好的特性吸引了用户：与 TensorFlow 需要在操作之前定义静态计算图不同，PyTorch 允许用户在操作过程中定义计算，这一方式后来也被 TensorFlow 的后续版本集成。虽然 TensorFlow 在工业界更为广泛使用，但 PyTorch 通常是研究人员更喜欢的机器学习框架。如果你想了解两者之间的更多差异，可以查看这篇博客文章。\n",
        "\n",
        "Now that we have learned enough about the background of `PyTorch`, let's start by importing it into our notebook. To install `PyTorch`, you can follow the instructions here. Alternatively, you can open this notebook using `Google Colab`, which already has `PyTorch` installed in its base kernel. Once you are done with the installation process, run the following cell:\n",
        "\n",
        "现在我们已经了解了 PyTorch 的背景，让我们开始在我们的笔记本中导入它。要安装 PyTorch，你可以按照此处的说明进行操作。或者，你可以使用 Google Colab 打开此笔记本，Google Colab 的基础内核已经安装了 PyTorch。安装完成后，运行以下单元格："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "u0ukr7quvrMx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import pprint, module we use for making our print statements prettier\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k10ZRdcBwDP3"
      },
      "source": [
        "We are all set to start our tutorial. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLdSN9ZXvrM0"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "Tensors are the most basic building blocks in `PyTorch`.  Tensors are similar to matrices, but the have extra properties and they can represent higher dimensions. For example, an square image with 256 pixels in both sides can be represented by a `3x256x256` tensor, where the first 3 dimensions represent the color channels, red, green and blue.\n",
        "\n",
        "张量是 PyTorch 中最基本的构建块。张量类似于矩阵，但具有额外的属性，可以表示更高的维度。例如，一张边长为 256 像素的正方形图像可以用一个 3x256x256 的张量来表示，其中前 3 个维度代表颜色通道：红色、绿色和蓝色。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aWvTEy7lgG"
      },
      "source": [
        "### Tensor Initialization\n",
        "There are several ways to instantiate tensors in `PyTorch`, which we will go through next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78_3AMEyvJd"
      },
      "source": [
        "#### **From a Python List**\n",
        "\n",
        "We can initalize a tensor from a `Python` list, which could include sublists. The dimensions and the data types will be automatically inferred by `PyTorch` when we use [`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html).\n",
        "\n",
        "我们可以从一个 Python 列表初始化一个张量，这个列表可以包含子列表。当我们使用 torch.tensor() 时，PyTorch 会自动推断张量的维度和数据类型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsjIW9I_ztiO",
        "outputId": "b78c8e03-211d-499b-9089-262f7ac04b19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor from a Python List\n",
        "data = [\n",
        "        [0, 1],\n",
        "        [2, 3],\n",
        "        [4, 5]\n",
        "       ]\n",
        "x_python = torch.tensor(data)\n",
        "\n",
        "# Print the tensor\n",
        "x_python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv6ZEoZ0RWb5"
      },
      "source": [
        "We can also call `torch.tensor()` with the optional `dtype` parameter, which will set the data type. Some useful datatypes to be familiar with are: `torch.bool`, `torch.float`, and `torch.long`.\n",
        "\n",
        "我们也可以使用可选的 dtype 参数来调用 torch.tensor()，这将设置数据类型。一些常用的数据类型有：torch.bool、torch.float 和 torch.long。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bQF5IhsD7-n",
        "outputId": "a4729524-2b06-4545-cf1f-f83a5a7574b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are using the dtype to create a tensor of particular type\n",
        "x_float = torch.tensor(data, dtype=torch.float)\n",
        "x_float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16VoILaaE-_j",
        "outputId": "8d236b7a-b60e-482c-c28b-df25925d7819"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[False,  True],\n",
              "        [ True,  True],\n",
              "        [ True,  True]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are using the dtype to create a tensor of particular type\n",
        "x_bool = torch.tensor(data, dtype=torch.bool)\n",
        "x_bool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4HccPWFEQUB"
      },
      "source": [
        "We can also get the same tensor in our specified data type using methods such as `float()`, `long()` etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_yq0SuTS_W",
        "outputId": "f01ac372-5cb4-4be7-ae34-7c0a37b1b444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_python.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFiS1OFdTlKE"
      },
      "source": [
        "We can also use `tensor.FloatTensor`, `tensor.LongTensor`, `tensor.Tensor` classes to instantiate a tensor of particular type. `LongTensor`s are particularly important in NLP as many methods that deal with indices require the indices to be passed as a `LongTensor`, which is a 64 bit integer.\n",
        "\n",
        "我们还可以使用 tensor.FloatTensor、tensor.LongTensor、tensor.Tensor 类来实例化特定类型的张量。LongTensor 在自然语言处理（NLP）中特别重要，因为许多处理索引的方法要求将索引作为 LongTensor（即 64 位整数）传递。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXXWZ1H2TkNN",
        "outputId": "66564be9-f6df-4fab-c248-724635a2e272"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 1.],\n",
              "        [2., 3.],\n",
              "        [4., 5.]])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# `torch.Tensor` defaults to float\n",
        "# Same as torch.FloatTensor(data)\n",
        "x = torch.Tensor(data)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuLzDzsoytM2"
      },
      "source": [
        "#### **From a NumPy Array**\n",
        "We can also initialize a tensor from a `NumPy` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtSNe8X-2Pox",
        "outputId": "e0e2222c-27db-485f-a005-9ee3f6463e72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5]], dtype=torch.int32)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize a tensor from a NumPy array\n",
        "ndarray = np.array(data)\n",
        "x_numpy = torch.from_numpy(ndarray)\n",
        "\n",
        "# Print the tensor\n",
        "x_numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhtcBgum3OZ3"
      },
      "source": [
        "#### **From a Tensor**\n",
        "We can also initialize a tensor from another tensor, using the following methods:\n",
        "\n",
        "* `torch.ones_like(old_tensor)`: Initializes a tensor of `1s`.\n",
        "* `torch.zeros_like(old_tensor)`: Initializes a tensor of `0s`.\n",
        "* `torch.rand_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a uniform distribution between `0` and `1`.\n",
        "* `torch.randn_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a normal distribution.\n",
        "\n",
        "All of these methods preserve the tensor properties of the original tensor passed in, such as the `shape` and `device`, which we will cover in a bit.\n",
        "\n",
        "我们还可以使用以下方法从另一个张量初始化一个新的张量：\n",
        "\n",
        "* `torch.ones_like(old_tensor)`：初始化一个全为 `1` 的张量。\n",
        "* `torch.zeros_like(old_tensor)`：初始化一个全为 `0` 的张量。\n",
        "* `torch.rand_like(old_tensor)`：初始化一个所有元素均从 `0` 到 `1` 的均匀分布中采样的张量。\n",
        "* `torch.randn_like(old_tensor)`：初始化一个所有元素均从正态分布中采样的张量。\n",
        "\n",
        "所有这些方法都会保留传入的原始张量的属性，如 `shape` 和 `device`，这些我们稍后会讲到。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoKVhcLh2yqe",
        "outputId": "8ed9acbe-947b-4890-db81-c83d802c0f86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a base tensor\n",
        "x = torch.tensor([[1., 2], [3, 4]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FncfGN6z7ELA",
        "outputId": "71db6165-ea7a-4068-da13-c0310790c4b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor of 0s\n",
        "x_zeros = torch.zeros_like(x)\n",
        "x_zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D993dpnP6iA8",
        "outputId": "ca8a7fd3-e7d0-47f7-a7c0-1bddc5614223"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor of 1s\n",
        "x_ones = torch.ones_like(x)\n",
        "x_ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBUDeEm97IqW",
        "outputId": "455a2ce8-caf6-4252-c1be-6207e81d2649"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.1556, 0.8162],\n",
              "        [0.6770, 0.8598]])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor where each element is sampled from a uniform distribution\n",
        "# between 0 and 1\n",
        "x_rand = torch.rand_like(x)\n",
        "x_rand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYsE3lKt7IEX",
        "outputId": "2c49ce88-937e-4afe-eb1c-37a625cbc77f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.8337,  1.0947],\n",
              "        [ 1.7748,  2.2634]])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a tensor where each element is sampled from a normal distribution\n",
        "x_randn = torch.randn_like(x)\n",
        "x_randn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6tqf7v38vbi"
      },
      "source": [
        "#### **By Specifying a Shape**\n",
        "We can also instantiate tensors by specifying their shapes (which we will cover in more detail in a bit). The methods we could use follow the ones in the previous section:\n",
        "\n",
        "我们还可以通过指定形状来实例化张量（稍后我们将详细介绍）。我们可以使用的方法遵循上一节中的方法：\n",
        "\n",
        "* `torch.zeros()`\n",
        "* `torch.ones()`\n",
        "* `torch.rand()`\n",
        "* `torch.randn()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh4I4Npz-dZ4",
        "outputId": "e45a1095-0d18-4266-ceb7-a7c9de1f18e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 2x3x2 tensor of 0s\n",
        "shape = (4, 2, 2)\n",
        "x_zeros = torch.zeros(shape) # x_zeros = torch.zeros(4, 3, 2) is an alternative\n",
        "x_zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEjeR24MLkN"
      },
      "source": [
        "#### **With `torch.arange()`**\n",
        "We can also create a tensor with `torch.arange(end)`, which returns a `1-D` tensor with elements ranging from `0` to `end-1`. We can use the optional `start` and `step` parameters to create tensors with different ranges.  \n",
        "\n",
        "我们还可以使用 `torch.arange(end)` 创建一个张量，这将返回一个包含从 `0` 到 `end-1` 元素的一维张量。我们可以使用可选的 `start` 和 `step` 参数来创建具有不同范围的张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EjARl2aM7pA",
        "outputId": "264a9ffa-5d05-4b9d-9901-cd49cbb93264"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor with values 0-9\n",
        "x = torch.arange(10)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgpkRn527zSr"
      },
      "source": [
        "### Tensor Properties\n",
        "\n",
        "Tensors have a few properties that are important for us to cover. These are namely `shape`, and the `device` properties.\n",
        "\n",
        "张量有几个重要的属性需要我们去了解。这些属性分别是“形状”和“设备”属性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBt6e4xZT3zr"
      },
      "source": [
        "#### Data Type\n",
        "\n",
        "The `dtype` property lets us see the data type of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlF3k3eUT_hQ",
        "outputId": "c2416faf-77cc-46e9-a22e-a67fe14bc13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
        "x = torch.ones(3, 2)\n",
        "print(x.dtype)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1vtN4Dy8FAG"
      },
      "source": [
        "#### Shape\n",
        "\n",
        "The `shape` property tells us the shape of our tensor. This can help us identify how many dimensional our tensor is as well as how many elements exist in each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24gXLJcn7Pxs",
        "outputId": "ef11b998-9fc4-458e-8452-6d545da19694"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV0cE1cXAEHP",
        "outputId": "1d12cf8d-c659-4127-807e-91459ba12d3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out its shape\n",
        "# Same as x.size()\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vnJnaAQlc",
        "outputId": "cfa251e1-be1c-4dbc-9803-bc213ca05a2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out the number of elements in a particular dimension\n",
        "# 0th dimension corresponds to the rows\n",
        "x.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxXCX6y6BvhH"
      },
      "source": [
        "We can also get the size of a particular dimension with the `size()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZapQmydxBVuy",
        "outputId": "24547ccf-817c-44ef-bc6a-03a0bf246a58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the size of the 0th dimension\n",
        "x.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCQm7ToPOveH"
      },
      "source": [
        "We can change the shape of a tensor with the `view()` method.\n",
        "\n",
        "我们可以使用“view()”方法改变张量的形状。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1JH3fiNO5Gu",
        "outputId": "0c270810-811b-466f-a5d2-70f240942aa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example use of view()\n",
        "# x_view shares the same memory as x, so changing one changes the other\n",
        "x_view = x.view(2, 3)\n",
        "x_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在 PyTorch 中，我们可以使用 -1 来让 PyTorch 推断某个维度的大小。具体来说，当你想要对一个张量进行重新调整形状（即重新排列其元素以形成一个新的形状）时，如果不确定某个维度的大小，可以将该维度指定为 -1，PyTorch 会根据张量的总元素数和其他维度的大小自动计算出这个维度的大小。\n",
        "\n",
        "在这个例子中，x.view(3, -1) 的作用是将张量 x 重新调整形状，使得新张量有 3 行，每行的元素个数由 PyTorch 根据 x 的总元素数来推断。\n",
        "\n",
        "假设 x 的形状是 (6, 4)，即 x 有 24 个元素。那么 x.view(3, -1) 会将 x 调整为一个形状为 (3, 8) 的张量，因为 3 * 8 = 24。\n",
        "\n",
        "这种方法在不改变数据顺序的情况下调整张量形状，非常方便处理高维数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3x4seqPGEI",
        "outputId": "66e48d01-5702-414c-c628-055adbdcdeef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can ask PyTorch to infer the size of a dimension with -1\n",
        "x_view = x.view(3, -1)\n",
        "x_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYSCEesPITpf"
      },
      "source": [
        "We can also use `torch.reshape()` method for a similar purpose. There is a subtle difference between `reshape()` and `view()`: `view()` requires the data to be stored contiguously in the memory. You can refer to [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) StackOverflow answer for more information. In simple terms, contiguous means that the way our data is laid out in the memory is the same as the way we would read elements from it. This happens because some methods, such as `transpose()` and `view()`, do not actually change how our data is stored in the memory. They just change the meta information about out tensor, so that when we use it we will see the elements in the order we expect.\n",
        "\n",
        "我们还可以使用 torch.reshape() 方法来实现类似的目的。reshape() 和 view() 之间有一个细微的区别：view() 要求数据在内存中是连续存储的。你可以参考 这篇 StackOverflow 回答获取更多信息。简单来说，连续存储意味着数据在内存中的布局方式与我们从中读取元素的方式是一致的。这是因为某些方法，比如 transpose() 和 view()，实际上并不会改变数据在内存中的存储方式，它们只是改变了张量的元信息，以便我们使用时按照我们期望的顺序查看元素。\n",
        "\n",
        "`reshape()` calls `view()` internally if the data is stored contiguously, if not, it returns a copy. The difference here isn't too important for basic tensors, but if you perform operations that make the underlying storage of the data non-contiguous (such as taking a transpose), you will have issues using `view()`. If you would like to match the way your tensor is stored in the memory to how it is used, you can use the `contiguous()` method.  \n",
        "\n",
        "如果数据在内存中是连续存储的，reshape() 在内部调用 view()，如果不是，它会返回一个复制品。对于基本的张量来说，这种差异并不太重要，但如果进行了使数据的底层存储非连续的操作（如取转置），使用 view() 就会出现问题。如果希望使张量在内存中的存储方式与使用方式匹配，可以使用 contiguous() 方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLGcGYE4Llom",
        "outputId": "e810557b-552f-4b0f-a37a-1924bec7fcfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Change the shape of x to be 3x2\n",
        "# x_reshaped could be a reference to or copy of x\n",
        "x_reshaped = torch.reshape(x, (2, 3))\n",
        "x_reshaped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWNTZKZZQ9i6"
      },
      "source": [
        "We can use `torch.unsqueeze(x, dim)` function to add a dimension of size `1` to the provided `dim`, where `x` is the tensor. We can also use the corresponding use `torch.squeeze(x)`, which removes the dimensions of size `1`.\n",
        "\n",
        "我们可以使用 torch.unsqueeze(x, dim) 函数在张量 x 的指定维度 dim 上添加一个大小为 1 的维度。我们也可以使用对应的 torch.squeeze(x) 函数，它会移除张量 x 中大小为 1 的维度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_IYojrJRh-m",
        "outputId": "ed315529-ea33-4cbe-9878-4581f3afbcdb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5],\n",
              "        [6, 7],\n",
              "        [8, 9]])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize a 5x2 tensor, with 5 rows and 2 columns\n",
        "x = torch.arange(10).reshape(5, 2)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLhg_oZ4SHh-",
        "outputId": "a32cf8a3-c1f4-4220-d948-b42fd12f8772"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 2])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add a new dimension of size 1 at the 1st dimension\n",
        "# 这里unsqueeze函数中传的参数指定了在第一维加一个新维度\n",
        "x = x.unsqueeze(1)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoGYGbMRSo-J",
        "outputId": "9c8415c4-ebf3-40ab-9faa-b13c5fd42c62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 2])"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\n",
        "# 通过删除所有只有 1 个元素的维度来压缩 x 的维度\n",
        "x = x.squeeze()\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQpZ4556B3lb"
      },
      "source": [
        "If we want to get the total number of elements in a tensor, we can use the `numel()` method.\n",
        "\n",
        "如果我们想要获取张量中元素的总数，可以使用 `numel()` 方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-irUWlxTB6a",
        "outputId": "c070c19a-cf0c-46cb-ec54-8254a9c83974"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [2, 3],\n",
              "        [4, 5],\n",
              "        [6, 7],\n",
              "        [8, 9]])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76yVMMg_CA0Q",
        "outputId": "7edfbc68-b830-459c-a5bf-fe3803a902af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the number of elements in tensor.\n",
        "x.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M1U_RTpBhl2"
      },
      "source": [
        "#### **Device**\n",
        "Device property tells `PyTorch` where to store our tensor. Where a tensor is stored determines which device, `GPU` or `CPU`, would be handling the computations involving it. We can find the device of a tensor with the `device` property.\n",
        "\n",
        "`Device` 属性告诉 `PyTorch` 应该将张量存储在哪里。张量存储的位置决定了处理与其相关的计算的设备，可以是 `GPU` 或者 `CPU`。我们可以通过 `device` 属性查看张量所在的设备。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYRGhIbnCl3b",
        "outputId": "4eac92c3-48f5-4e4e-cdf1-329bd79538b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.Tensor([[1, 2], [3, 4]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byEnJyKdBgjl",
        "outputId": "04440390-a16d-4250-920e-64dade75622a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the device of the tensor\n",
        "x.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vqf_-NADFX8"
      },
      "source": [
        "We can move a tensor from one device to another with the method `to(device)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "GzA6zqkXDEt1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Check if a GPU is available, if so, move the tensor to the GPU\n",
        "if torch.cuda.is_available():\n",
        "  x = x.to('cuda')\n",
        "\n",
        "print(x.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7BMktFFAkRA"
      },
      "source": [
        "### Tensor Indexing\n",
        "In `PyTorch` we can index tensors, similar to `NumPy`.\n",
        "\n",
        "在“PyTorch”中，我们可以索引张量，类似于“NumPy”。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRJN7ovWDsKV",
        "outputId": "f4bea081-d4c4-49a2-e529-fa502cf135f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  2.],\n",
              "         [ 3.,  4.]],\n",
              "\n",
              "        [[ 5.,  6.],\n",
              "         [ 7.,  8.]],\n",
              "\n",
              "        [[ 9., 10.],\n",
              "         [11., 12.]]])"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.Tensor([\n",
        "                  [[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]],\n",
        "                  [[9, 10], [11, 12]]\n",
        "                 ])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M67ZiOF1Heyc",
        "outputId": "0821f0cc-5a93-46e6-bee4-9fd648723a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2])"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guXKE7m8AX1K",
        "outputId": "cf492c27-a67d-4fbe-b57a-54fb4f8802a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the 0th element, which is the first row\n",
        "x[0] # Equivalent to x[0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8m8EyVvES4-"
      },
      "source": [
        "We can also index into multiple dimensions with `:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z6GFUcuEL85",
        "outputId": "f36a740c-df12-42f6-a64b-d4b3038168f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 5., 9.])"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the top left element of each element in our tensor\n",
        "x[:, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm8vc3nuXaEw"
      },
      "source": [
        "We can also access arbitrary elements in each dimension.\n",
        "\n",
        "我们还可以访问每个维度中的任意元素。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYhcH9gaWHyW",
        "outputId": "f900ea02-4f55-4af6-c485-c5dec4baf196"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  2.],\n",
              "         [ 3.,  4.]],\n",
              "\n",
              "        [[ 5.,  6.],\n",
              "         [ 7.,  8.]],\n",
              "\n",
              "        [[ 9., 10.],\n",
              "         [11., 12.]]])"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print x again to see our tensor\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4xl6CW3RrEw",
        "outputId": "1cb93869-7764-4d22-b411-dcaadb906830"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]]])"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's access the 0th and 1st elements, each twice\n",
        "i = torch.tensor([0, 0, 1, 1])\n",
        "x[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 1, 0]\n",
            "torch.Size([3, 2, 2])\n",
            "tensor([[[ 9., 10.],\n",
            "         [11., 12.]],\n",
            "\n",
            "        [[ 5.,  6.],\n",
            "         [ 7.,  8.]],\n",
            "\n",
            "        [[ 1.,  2.],\n",
            "         [ 3.,  4.]]])\n"
          ]
        }
      ],
      "source": [
        "i = [2 - i for i in range(3)]\n",
        "print(i)\n",
        "print(x.shape)\n",
        "print(x[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3QYZ8k7Wvqp",
        "outputId": "6cb46909-2800-45a0-abb3-0707c2c50d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.,  2.],\n",
            "         [ 3.,  4.]],\n",
            "\n",
            "        [[ 5.,  6.],\n",
            "         [ 7.,  8.]],\n",
            "\n",
            "        [[ 9., 10.],\n",
            "         [11., 12.]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's access the 0th elements of the 1st and 2nd elements\n",
        "i = torch.tensor([1, 2])\n",
        "j = torch.tensor([0])\n",
        "print(x)\n",
        "x[i, j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAELXC--IHS7"
      },
      "source": [
        "We can get a `Python` scalar value from a tensor with `item()`.\n",
        "\n",
        "我们可以使用“item()”从张量中获取“Python”标量值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM-ZujN2IGaQ",
        "outputId": "785f43c3-5670-4fa2-ed51-e72e9c0474ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NwxK7d_Ycgs",
        "outputId": "343abf96-2d77-49b0-9076-c0f617e7aafe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0, 0, 0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GltnmDzeIXJM"
      },
      "source": [
        "### Operations\n",
        "PyTorch operations are very similar to those of `NumPy`. We can work with both scalars and other tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9KBzcA0G6v9",
        "outputId": "057decb7-7a56-4fcc-bc20-781cba8d0caf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 1.],\n",
              "         [1., 1.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [1., 1.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [1., 1.]]])"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an example tensor\n",
        "x = torch.ones((3,2,2))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUw8MAHqKuzs",
        "outputId": "fd10173a-caf5-4e45-ceca-c0b32f4bf39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[3., 3.],\n",
              "         [3., 3.]],\n",
              "\n",
              "        [[3., 3.],\n",
              "         [3., 3.]],\n",
              "\n",
              "        [[3., 3.],\n",
              "         [3., 3.]]])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform elementwise addition\n",
        "# Use - for subtraction\n",
        "x + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAfMsaz1Gw5v",
        "outputId": "d0432152-8137-46e7-902a-ec0b779d5471"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[2., 2.],\n",
              "         [2., 2.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [2., 2.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [2., 2.]]])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform elementwise multiplication\n",
        "# Use / for division\n",
        "x * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aq89FU7OOe7"
      },
      "source": [
        "We can apply the same operations between different tensors of compatible sizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGhz62wILIfN",
        "outputId": "40f51f08-a80b-41b9-b9a1-e874c8b89e05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.]])"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a 4x3 tensor of 6s\n",
        "a = torch.ones((4,3)) * 6\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvDC1OzzPyLV",
        "outputId": "291f0c1f-9a64-4c23-a399-7464c6cbd39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2., 2., 2.])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a 1D tensor of 2s\n",
        "b = torch.ones(3) * 2\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUF9noMTP5NI",
        "outputId": "9cd9b5ab-c292-4987-c562-a3f8e4ec8a2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[3., 3., 3.],\n",
              "        [3., 3., 3.],\n",
              "        [3., 3., 3.],\n",
              "        [3., 3., 3.]])"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Divide a by b\n",
        "a / b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTiVVbukXRct"
      },
      "source": [
        "We can use `tensor.matmul(other_tensor)` for matrix multiplication and `tensor.T` for transpose. Matrix multiplication can also be performed with `@`.\n",
        "\n",
        "我们可以使用 `tensor.matmul(other_tensor)` 进行矩阵乘法，使用 `tensor.T` 进行转置操作。矩阵乘法也可以使用 `@` 符号来执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPoC_WcbXCw5",
        "outputId": "332f95f5-11fd-431d-c77a-e4c98dbd6660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([36., 36., 36., 36.])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Alternative to a.matmul(b)\n",
        "# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n",
        "# is inferred\n",
        "a @ b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2WEoQjVYBQ8",
        "outputId": "0f97c447-5f28-4483-ac82-e1b484663b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3])\n",
            "torch.Size([3, 4])\n"
          ]
        }
      ],
      "source": [
        "pp.pprint(a.shape)\n",
        "pp.pprint(a.T.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PibNpxbYYf2"
      },
      "source": [
        "We can take the mean and standard deviation along a certain dimension with the methods `mean(dim)` and `std(dim)`. That is, if we want to get the mean `3x2` matrix in a `4x3x2` matrix, we would set the `dim` to be 0. We can call these methods with no parameter to get the mean and standard deviation for the whole tensor. To use `mean` and `std` our tensor should be a floating point type.\n",
        "\n",
        "我们可以使用方法 `mean(dim)` 和 `std(dim)` 沿着指定的维度计算均值和标准差。例如，如果我们想在一个 `4x3x2` 的张量中获取一个 `3x2` 矩阵的均值，我们可以将 `dim` 设置为 0。我们也可以不传递参数来调用这些方法，从而计算整个张量的均值和标准差。使用 `mean` 和 `std` 方法时，我们的张量应该是浮点类型的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a987teCtYg7R",
        "outputId": "0ca3799b-6dec-49ea-c1eb-d6265f390818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Mean: 2.5'\n",
            "'Mean in the 0th dimension: tensor([2.5000, 2.5000])'\n",
            "'Mean in the 1st dimension: tensor([1., 2., 3., 4.])'\n"
          ]
        }
      ],
      "source": [
        "# Create an example tensor\n",
        "m = torch.tensor(\n",
        "    [\n",
        "     [1., 1.],\n",
        "     [2., 2.],\n",
        "     [3., 3.],\n",
        "     [4., 4.]\n",
        "    ]\n",
        ")\n",
        "\n",
        "pp.pprint(\"Mean: {}\".format(m.mean()))\n",
        "pp.pprint(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n",
        "pp.pprint(\"Mean in the 1st dimension: {}\".format(m.mean(1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd77stQ5VQVT"
      },
      "source": [
        "We can concatenate tensors using `torch.cat`.\n",
        "\n",
        "我们可以使用“torch.cat”连接张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "advfDCOPK9Gw",
        "outputId": "8efcab75-98f0-4562-a58f-fa419604956b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.],\n",
            "        [6., 6., 6.],\n",
            "        [6., 6., 6.]])\n",
            "Initial shape: torch.Size([4, 3])\n",
            "Shape after concatenation in dimension 0: torch.Size([12, 3])\n",
            "Shape after concatenation in dimension 1: torch.Size([4, 9])\n"
          ]
        }
      ],
      "source": [
        "# Concatenate in dimension 0 and 1\n",
        "print(a)\n",
        "a_cat0 = torch.cat([a, a, a], dim=0)\n",
        "a_cat1 = torch.cat([a, a, a], dim=1)\n",
        "\n",
        "print(\"Initial shape: {}\".format(a.shape))\n",
        "print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n",
        "print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BveswZMOjtff"
      },
      "source": [
        "Most of the operations in `PyTorch` are not in place. However, `PyTorch` offers the in place versions of operations available by adding an underscore (`_`) at the end of the method name.\n",
        "\n",
        "在 PyTorch 中，大多数操作都不是原地操作。然而，PyTorch 提供了原地操作的版本，通过在方法名后面添加下划线 (`_`) 来使用这些版本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ebr7nn-DaU3B",
        "outputId": "aad366a1-1bbb-4864-8be0-46ef440763b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.]])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print our tensor\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP8-VtoHaKAc",
        "outputId": "94fafdfd-da16-4a94-bdd3-f9935a278ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.],\n",
              "        [6., 6., 6.]])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add() is not in place\n",
        "a.add(a)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mY0ojINbaayp",
        "outputId": "cd396248-e6e4-4d0b-ebf9-5a7411f27882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6., 6., 6.],\n",
            "        [6., 6., 6.],\n",
            "        [6., 6., 6.],\n",
            "        [6., 6., 6.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12.],\n",
              "        [12., 12., 12.],\n",
              "        [12., 12., 12.],\n",
              "        [12., 12., 12.]])"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add_() is in place\n",
        "print(a)\n",
        "a.add_(a)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re8xiL37eAja"
      },
      "source": [
        "## Autograd\n",
        "`PyTorch` and other machine learning libraries are known for their automatic differantiation feature. That is, given that we have defined the set of operations that need to be performed, the framework itself can figure out how to compute the gradients. We can call the `backward()` method to ask `PyTorch` to calculate the gradiends, which are then stored in the `grad` attribute.\n",
        "\n",
        "“PyTorch”和其他机器学习库以其自动微分功能而闻名，也就是说，只要我们定义了需要执行的操作集，框架本身就可以计算出如何计算梯度。 “backward()”方法要求“PyTorch”计算梯度，然后将其存储在“grad”属性中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oEvBJHWfn8H",
        "outputId": "740dc424-2b22-4d2b-ac2a-6d45e06d914b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在PyTorch中，默认情况下，每次调用 .backward() 方法时，梯度会累加到张量的 .grad 属性中。这意味着如果一个张量已经具有非零的梯度，并且再次调用 .backward() 方法，新的梯度将会被加到现有梯度上，而不是替换掉原有的梯度。\n",
        "\n",
        "这种行为对于训练神经网络特别有用，因为通常在一个批次（batch）中计算多个样本的损失函数，然后将它们的梯度累积起来，最后更新模型参数。如果不想累积梯度，可以在每次 .backward() 调用前先将 .grad 属性置零。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTJazZXkgthP",
        "outputId": "e3d72745-0206-4c31-e11e-7b7ad833d5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2.], requires_grad=True)\n",
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "# x.grad.zero_()\n",
        "y.backward()\n",
        "print(x)\n",
        "pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hqc2oM3iV6a"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--Az0Xiic_z",
        "outputId": "21195fa0-8466-4a33-92df-be40e3c33745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2.], requires_grad=True)\n",
            "tensor([24.])\n"
          ]
        }
      ],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "print(x)\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhjPkiE6i7ja"
      },
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong.\n",
        "\n",
        "\n",
        "我们可以看到 x.grad 被更新为到目前为止计算得到的梯度之和。当我们在神经网络中运行反向传播时，我们会在更新之前将某个神经元的所有梯度加起来。这就是这里发生的情况！这也是为什么我们需要在每个训练迭代中运行 zero_grad() 的原因（稍后会详细讨论）。否则，我们的梯度会从一个训练迭代积累到另一个，导致我们的更新出错。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYLWqKIoaOyd"
      },
      "source": [
        "## Neural Network Module\n",
        "\n",
        "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 3, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it.\n",
        "\n",
        "到目前为止，我们已经研究了张量、它们的属性以及张量的基本操作。如果我们打算从头开始构建网络的各个层，这些内容尤其有用。在作业3中，我们将利用这些知识。然而，往后我们会使用 PyTorch 的 `torch.nn` 模块中预定义的模块。我们会将这些模块组合起来创建复杂的网络结构。让我们从导入这个模块开始，并使用别名，这样每次使用时就不需要输入 `torch`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "qUmrDpbhV4Tn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joGvRWjEbak0"
      },
      "source": [
        "### **Linear Layer** 线性层\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`.\n",
        "\n",
        "我们可以使用 `nn.Linear(H_in, H_out)` 来创建一个线性层。这将接受维度为 `(N, *, H_in)` 的矩阵作为输入，并输出维度为 `(N, *, H_out)` 的矩阵。这里的 `*` 表示中间可以有任意数量的维度。线性层执行的操作是 `Ax+b`，其中 `A` 和 `b` 是随机初始化的参数。如果我们不希望线性层学习偏置参数，可以使用 `bias=False` 来初始化我们的层。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XfnKI4-a5j9",
        "outputId": "867af230-5f0a-4b0b-c06c-025b1b8b0b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the input is tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]],\n",
            "\n",
            "        [[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[-0.2490, -0.0327],\n",
            "         [-0.2490, -0.0327],\n",
            "         [-0.2490, -0.0327]],\n",
            "\n",
            "        [[-0.2490, -0.0327],\n",
            "         [-0.2490, -0.0327],\n",
            "         [-0.2490, -0.0327]]], grad_fn=<ViewBackward0>)\n",
            "the input's shape is torch.Size([2, 3, 4])\n",
            "the output's shape is torch.Size([2, 3, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "# N* H_in -> N*H_out\n",
        "\n",
        "\n",
        "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
        "# dimensional outputs\n",
        "linear = nn.Linear(4, 2)\n",
        "# linear = nn.Linear(4, 2, bias=False)\n",
        "# nn.Linear(2,1)\n",
        "linear_output = linear(input)\n",
        "print(\"the input is {}\".format(input))\n",
        "print(linear_output)\n",
        "print(\"the input's shape is {}\".format(input.shape))\n",
        "print(\"the output's shape is {}\".format(linear_output.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "上述代码将返回一个包含两个元素的列表 params，其中：\n",
        "\n",
        "params[0] 是权重矩阵 A，维度为 (H_out, H_in)，对于上面的示例，是 (2, 4)。\n",
        "\n",
        "params[1] 是偏置向量 b，维度为 (H_out,)，对于上面的示例，是 (2,)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_9XKtAFYpdI",
        "outputId": "df248f31-e5ca-4857-88c2-c0bbc61a40be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.4654, -0.1241,  0.1760,  0.2889],\n",
              "         [-0.4991,  0.3690, -0.3155, -0.0010]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1244,  0.4139], requires_grad=True)]"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(linear.parameters()) # Ax + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXCCu9keUlW"
      },
      "source": [
        "### **Other Module Layers**\n",
        "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up.\n",
        "\n",
        "在 `nn` 模块中还有几个预配置的层。一些常用的示例包括 `nn.Conv2d`、`nn.ConvTranspose2d`、`nn.BatchNorm1d`、`nn.BatchNorm2d`、`nn.Upsample` 和 `nn.MaxPool2d` 等等。随着课程的进展，我们会更多地了解这些层。目前，唯一需要记住的重要事情是，我们可以将每个这些层视为即插即用的组件：我们只需提供所需的维度，`PyTorch` 就会负责设置它们。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yslDOK66fYWn"
      },
      "source": [
        "### **Activation Function Layer** 激活层\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in.\n",
        "\n",
        "我们也可以使用 `nn` 模块将激活函数应用到我们的张量上。激活函数用于给网络增加非线性。一些常见的激活函数包括 `nn.ReLU()`、`nn.Sigmoid()` 和 `nn.LeakyReLU()`。激活函数对每个元素进行操作，因此输出张量的形状与输入张量相同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrJP5CveeOON",
        "outputId": "ce8b78e3-6311-44e2-c40c-f09f81f1acd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2490, -0.0327],\n",
              "         [-0.2490, -0.0327],\n",
              "         [-0.2490, -0.0327]],\n",
              "\n",
              "        [[-0.2490, -0.0327],\n",
              "         [-0.2490, -0.0327],\n",
              "         [-0.2490, -0.0327]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9v5FjQtd4Ck",
        "outputId": "966538af-4eac-475b-dd30-9174efd7ed3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the input of sigmoid's shape is torch.Size([2, 3, 2])\n",
            "the output of sigmoid's shape is torch.Size([2, 3, 2])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[0.4381, 0.4918],\n",
              "         [0.4381, 0.4918],\n",
              "         [0.4381, 0.4918]],\n",
              "\n",
              "        [[0.4381, 0.4918],\n",
              "         [0.4381, 0.4918],\n",
              "         [0.4381, 0.4918]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "print(\"the input of sigmoid's shape is {}\".format(linear_output.shape))\n",
        "print(\"the output of sigmoid's shape is {}\".format(output.shape))\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiYTthJwhEYT"
      },
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that.\n",
        "\n",
        "到目前为止，我们已经看到，我们可以创建层并将一个层的输出作为下一个层的输入。我们可以使用 `nn.Sequentual`，而不是创建中间张量并传递它们，它就是这样做的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJeOqLxhBLY",
        "outputId": "b2370e4a-47bc-47f2-e784-d7fcd3ce57b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.3418, 0.3809],\n",
              "         [0.3418, 0.3809],\n",
              "         [0.3418, 0.3809]],\n",
              "\n",
              "        [[0.3418, 0.3809],\n",
              "         [0.3418, 0.3809],\n",
              "         [0.3418, 0.3809]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkJ81p3GUVPM"
      },
      "source": [
        "### Custom Modules 自定义模块\n",
        "\n",
        "Instead of using the predefined modules, we can also build our own by extending the `nn.Module` class. For example, we can build a the `nn.Linear` (which also extends `nn.Module`) on our own using the tensor introduced earlier! We can also build new, more complex modules, such as a custom neural network. You will be practicing these in the later assignment.\n",
        "\n",
        "除了使用预定义模块，我们还可以通过扩展 `nn.Module` 类来构建自己的模块。例如，我们可以使用前面介绍的张量自己构建一个类似于 `nn.Linear`（也是扩展自 `nn.Module` 的）的模块！我们还可以构建更复杂的新模块，比如自定义的神经网络。你将在后面的作业中练习这些内容。\n",
        "\n",
        "To create a custom module, the first thing we have to do is to extend the `nn.Module`. We can then initialize our parameters in the `__init__` function, starting with a call to the `__init__` function of the super class. All the class attributes we define which are `nn` module objects are treated as parameters, which can be learned during the training. Tensors are not parameters, but they can be turned into parameters if they are wrapped in `nn.Parameter` class.\n",
        "\n",
        "要创建一个自定义模块，第一步是扩展 `nn.Module`。然后我们可以在 `__init__` 函数中初始化参数，首先调用超类的 `__init__` 函数。我们定义的所有类属性，如果是 `nn` 模块对象，都会被视为参数，在训练过程中可以学习它们。张量不是参数，但如果它们被包装在 `nn.Parameter` 类中，就可以将其作为参数处理。\n",
        "\n",
        "All classes extending `nn.Module` are also expected to implement a `forward(x)` function, where `x` is a tensor. This is the function that is called when a parameter is passed to our module, such as in `model(x)`.\n",
        "\n",
        "所有扩展自 `nn.Module` 的类还应该实现一个 `forward(x)` 函数，其中 `x` 是一个张量。这个函数在将参数传递给我们的模块时调用，比如 `model(x)` 中的 `x`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "J2P7eZiMj32_"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    # 调用父类 nn.Module 的初始化方法，确保父类的初始化逻辑被执行\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    # 这两行代码将传入的 input_size 和 hidden_size 参数保存为实例属性，以便在定义模型时使用。\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining of our model\n",
        "    # There isn't anything specific about the naming of `self.model`. It could\n",
        "    # be something arbitrary.\n",
        "    # 定义我们的模型\n",
        "    # “self.model”的命名没有任何特殊之处。它可以是任意的。\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input_size, self.hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.hidden_size, self.input_size),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2DrfLiBVjNT"
      },
      "source": [
        "Here is an alternative way to define the same class. You can see that we can replace `nn.Sequential` by defining the individual layers in the `__init__` method and connecting the in the `forward` method.\n",
        "\n",
        "这是定义相同类的另一种方式。你可以看到，我们可以通过在 `__init__` 方法中定义各个层，并在 `forward` 方法中连接它们，来替换 `nn.Sequential`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "9-lqhsqwViIk"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    # Call to the __init__ function of the super class\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Bookkeeping: Saving the initialization parameters\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # Defining of our layers\n",
        "    self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    linear = self.linear(x)\n",
        "    relu = self.relu(linear)\n",
        "    linear2 = self.linear2(relu)\n",
        "    output = self.sigmoid(linear2)\n",
        "    print(\"forward function被自动执行了\")\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQelcFo5bXgU"
      },
      "source": [
        "Now that we have defined our class, we can instantiate it and see what it does.\n",
        "\n",
        "现在我们已经定义了我们的类，我们可以实例化它并看看它能做什么。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXi0T0FZbV0y",
        "outputId": "23547003-cfef-4a6d-e3f4-5fdb7a5cdd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the input is : \n",
            " tensor([[ 1.3909, -0.5912, -0.5251,  1.0540, -0.6228],\n",
            "        [ 1.0283,  0.0940, -0.1146,  0.2216,  0.2519]])\n",
            "forward function被自动执行了\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.5363, 0.4418, 0.5460, 0.5666, 0.5043],\n",
              "        [0.5305, 0.4534, 0.5292, 0.5707, 0.4894]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make a sample input\n",
        "input = torch.randn(2, 5)\n",
        "\n",
        "# Create our model\n",
        "model = MultilayerPerceptron(5, 3)\n",
        "\n",
        "print(\"the input is : \\n {}\".format(input))\n",
        "# Pass our input through our model\n",
        "model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCCbjc-Fb2-B"
      },
      "source": [
        "We can inspect the parameters of our model with `named_parameters()` and `parameters()` methods.\n",
        "\n",
        "我们可以使用 `named_parameters()` 和 `parameters()` 方法来检查模型的参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d23soYIb2WZ",
        "outputId": "3bf4243d-2f85-4593-8989-680b322aca13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.2844,  0.4322, -0.1225,  0.4251,  0.0993],\n",
            "        [-0.4250, -0.2914,  0.3961, -0.0591,  0.0406],\n",
            "        [-0.0911, -0.3171,  0.2475,  0.1379, -0.2612]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.3084, -0.0295, -0.3946], requires_grad=True)), ('linear2.weight', Parameter containing:\n",
            "tensor([[ 0.1882, -0.1808,  0.4383],\n",
            "        [-0.3787,  0.3676,  0.5601],\n",
            "        [ 0.5456,  0.0542,  0.5238],\n",
            "        [-0.1340,  0.0587,  0.2232],\n",
            "        [ 0.4793,  0.4065,  0.4155]], requires_grad=True)), ('linear2.bias', Parameter containing:\n",
            "tensor([ 0.0923, -0.1271,  0.0307,  0.3059, -0.1182], requires_grad=True))]\n",
            "--------------------------------------------------------------------------------\n",
            "[Parameter containing:\n",
            "tensor([[ 0.2844,  0.4322, -0.1225,  0.4251,  0.0993],\n",
            "        [-0.4250, -0.2914,  0.3961, -0.0591,  0.0406],\n",
            "        [-0.0911, -0.3171,  0.2475,  0.1379, -0.2612]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.3084, -0.0295, -0.3946], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.1882, -0.1808,  0.4383],\n",
            "        [-0.3787,  0.3676,  0.5601],\n",
            "        [ 0.5456,  0.0542,  0.5238],\n",
            "        [-0.1340,  0.0587,  0.2232],\n",
            "        [ 0.4793,  0.4065,  0.4155]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0923, -0.1271,  0.0307,  0.3059, -0.1182], requires_grad=True)]\n"
          ]
        }
      ],
      "source": [
        "print(list(model.named_parameters()))\n",
        "print(\"-\" * 80)\n",
        "print(list(model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5JegycOdMFy"
      },
      "source": [
        "## Optimization 优化\n",
        "We have showed how gradients are calculated with the `backward()` function. Having the gradients isn't enought for our models to learn. We also need to know how to update the parameters of our models. This is where the optomozers comes in. `torch.optim` module contains several optimizers that we can use. Some popular examples are `optim.SGD` and `optim.Adam`. When initializing optimizers, we pass our model parameters, which can be accessed with `model.parameters()`, telling the optimizers which values it will be optimizing. Optimizers also has a learning rate (`lr`) parameter, which determines how big of an update will be made in every step. Different optimizers have different hyperparameters as well.\n",
        "\n",
        "我们已经展示了如何使用 `backward()` 函数计算梯度。然而，仅有梯度并不足以让我们的模型学习。我们还需要知道如何更新模型的参数。这就是优化器发挥作用的地方。`torch.optim` 模块包含了多种优化器供我们使用。一些常见的例子包括 `optim.SGD` 和 `optim.Adam`。在初始化优化器时，我们通过传递模型的参数（可以通过 `model.parameters()` 访问）告诉优化器它将优化哪些值。优化器还有一个学习率 (`lr`) 参数，它决定每一步更新的幅度大小。不同的优化器还有不同的超参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "W0F-TvV0kk-I"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgak6o5dlQWF"
      },
      "source": [
        "After we have our optimization function, we can define a `loss` that we want to optimize for. We can either define the loss ourselves, or use one of the predefined loss function in `PyTorch`, such as `nn.BCELoss()`. Let's put everything together now! We will start by creating some dummy data.\n",
        "\n",
        "在我们拥有优化函数之后，我们可以定义一个要优化的损失函数 loss。我们可以自己定义损失函数，也可以使用 PyTorch 中预定义的损失函数之一，比如 nn.BCELoss()。现在让我们把所有的东西都放在一起吧！我们将从创建一些虚拟数据开始。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGYFiaT_vXBn",
        "outputId": "48bd9b70-f225-4c20-fad7-5f7a6fa4ed9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.5732,  0.3369,  1.5707, -0.6915, -0.0843],\n",
              "        [ 0.2842,  2.0114,  1.6029,  2.1764,  0.5551],\n",
              "        [ 1.3770,  0.2411,  2.5890,  1.6288,  0.8746],\n",
              "        [-0.6140,  2.5605,  0.2022,  0.1500,  0.3114],\n",
              "        [ 0.2285,  0.8403, -0.2322, -0.2084, -1.3668],\n",
              "        [ 0.8640,  0.4660,  1.3431,  0.2402,  0.6711],\n",
              "        [ 0.1234,  1.4809,  0.3616,  2.0273,  0.1224],\n",
              "        [ 1.0940, -0.2841,  2.4534,  0.7063,  1.2543],\n",
              "        [ 0.3050,  0.9035,  2.5951,  2.2284,  0.0268],\n",
              "        [-0.1267,  1.8280,  1.9639, -0.1851,  1.0359]])"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the y data\n",
        "y = torch.ones(10, 5)\n",
        "\n",
        "# Add some noise to our goal y to generate our x\n",
        "# We want out model to predict our original data, albeit the noise\n",
        "x = y + torch.randn_like(y)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEsiOdpWvfLj"
      },
      "source": [
        "Now, we can define our model, optimizer and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oA2XsdsbN8p",
        "outputId": "b4cd8c6c-d2e9-4594-f238-234d61e027d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward function被自动执行了\n",
            "the prediction value is :\n",
            " tensor([[0.3628, 0.4871, 0.4378, 0.4830, 0.4526],\n",
            "        [0.4081, 0.3948, 0.4663, 0.5676, 0.3965],\n",
            "        [0.3641, 0.4616, 0.4342, 0.5136, 0.4214],\n",
            "        [0.4125, 0.3839, 0.4686, 0.5785, 0.3882],\n",
            "        [0.4268, 0.4636, 0.4103, 0.5309, 0.4319],\n",
            "        [0.4057, 0.3973, 0.4645, 0.5660, 0.3966],\n",
            "        [0.4170, 0.3712, 0.4706, 0.5917, 0.3775],\n",
            "        [0.3458, 0.4832, 0.4191, 0.4997, 0.4223],\n",
            "        [0.3522, 0.4982, 0.4288, 0.4767, 0.4513],\n",
            "        [0.3931, 0.4277, 0.4576, 0.5361, 0.4189]], grad_fn=<SigmoidBackward0>)\n",
            "0.8197198510169983\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "model = MultilayerPerceptron(5, 3)\n",
        "\n",
        "# Define the optimizer\n",
        "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "# 使用预定义的二元交叉熵损失函数 (nn.BCELoss())。这个损失函数通常用于二分类任务，衡量模型预测与真实标签之间的差异\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# Calculate how our model is doing now\n",
        "# y_pred = model(x)：使用模型 model 对输入 x 进行预测，得到预测结果 y_pred\n",
        "y_pred = model(x)\n",
        "print(\"the prediction value is :\\n {}\".format(y_pred))\n",
        "\n",
        "# 计算预测值 y_pred 相对于真实标签 y 的损失值，并使用 .item() 方法将损失值转换为 Python 数值类型，方便后续打印或记录\n",
        "print(loss_function(y_pred, y).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxU7Y8ZufSR"
      },
      "source": [
        "Let's see if we can have our model achieve a smaller loss. Now that we have everything we need, we can setup our training loop.\n",
        "\n",
        "让我们看看我们能否让我们的模型实现更小的损失。现在我们已经拥有了所需的一切，我们可以设置我们的训练循环了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogl6-Ctmuek6",
        "outputId": "b22f36a0-c702-4c89-e5d4-27c60d27c0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward function被自动执行了\n",
            "Epoch 0: traing loss: 0.8197198510169983\n",
            "forward function被自动执行了\n",
            "Epoch 1: traing loss: 0.6946194171905518\n",
            "forward function被自动执行了\n",
            "Epoch 2: traing loss: 0.5555264353752136\n",
            "forward function被自动执行了\n",
            "Epoch 3: traing loss: 0.4089398980140686\n",
            "forward function被自动执行了\n",
            "Epoch 4: traing loss: 0.2753055691719055\n",
            "forward function被自动执行了\n",
            "Epoch 5: traing loss: 0.17014183104038239\n",
            "forward function被自动执行了\n",
            "Epoch 6: traing loss: 0.09768878668546677\n",
            "forward function被自动执行了\n",
            "Epoch 7: traing loss: 0.051497675478458405\n",
            "forward function被自动执行了\n",
            "Epoch 8: traing loss: 0.025548718869686127\n",
            "forward function被自动执行了\n",
            "Epoch 9: traing loss: 0.012314668856561184\n"
          ]
        }
      ],
      "source": [
        "# Set the number of epoch, which determines the number of training iterations\n",
        "n_epoch = 10\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  # 在每次反向传播计算梯度后，需要清除之前计算的梯度，以避免梯度的累积影响下一次的优化步骤。这就是使用 zero_grad() 方法的目的\n",
        "  adam.zero_grad()\n",
        "\n",
        "  # Get the model predictions\n",
        "  y_pred = model(x)\n",
        "\n",
        "  # Get the loss\n",
        "  loss = loss_function(y_pred, y)\n",
        "\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "\n",
        "  # Compute the gradients\n",
        "  # 反向传播计算梯度\n",
        "  loss.backward()\n",
        "\n",
        "  # Take a step to optimize the weights\n",
        "  # 用于执行优化步骤，即根据计算得到的梯度更新模型参数\n",
        "  adam.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrMJ8AmqeCY-",
        "outputId": "73815ecf-9630-4273-9cbb-516258bafe10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('linear.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[ 0.7638,  0.9341,  1.3337,  0.7523,  0.4563],\n",
              "          [-0.0690,  1.0464, -0.1449, -0.7780, -1.3294],\n",
              "          [-0.7790, -0.7387, -0.2126, -0.2951, -0.1450]], requires_grad=True)),\n",
              " ('linear.bias',\n",
              "  Parameter containing:\n",
              "  tensor([ 1.0171,  0.5077, -0.3609], requires_grad=True)),\n",
              " ('linear2.weight',\n",
              "  Parameter containing:\n",
              "  tensor([[0.8198, 1.2320, 0.0357],\n",
              "          [1.2306, 1.3758, 0.9619],\n",
              "          [0.8857, 0.4290, 0.1428],\n",
              "          [0.6065, 0.8632, 0.0661],\n",
              "          [1.1845, 1.0837, 0.5626]], requires_grad=True)),\n",
              " ('linear2.bias',\n",
              "  Parameter containing:\n",
              "  tensor([0.5321, 0.2765, 0.7522, 1.2645, 0.3139], requires_grad=True))]"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.named_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nXApd82wlsF"
      },
      "source": [
        "You can see that our loss is decreasing. Let's check the predictions of our model now and see if they are close to our original `y`, which was all `1s`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRqE7P9EtvuS",
        "outputId": "d4c76a74-0568-43a9-da28-f25340b87e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward function被自动执行了\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.9845, 0.9939, 0.9699, 0.9798, 0.9903],\n",
              "        [0.9983, 0.9999, 0.9992, 0.9963, 0.9998],\n",
              "        [0.9986, 0.9999, 0.9993, 0.9968, 0.9999],\n",
              "        [0.9987, 0.9997, 0.9931, 0.9966, 0.9993],\n",
              "        [0.9956, 0.9976, 0.9521, 0.9912, 0.9935],\n",
              "        [0.9842, 0.9966, 0.9904, 0.9807, 0.9960],\n",
              "        [0.9899, 0.9981, 0.9926, 0.9859, 0.9975],\n",
              "        [0.9956, 0.9995, 0.9976, 0.9925, 0.9994],\n",
              "        [0.9985, 0.9999, 0.9992, 0.9965, 0.9999],\n",
              "        [0.9980, 0.9998, 0.9977, 0.9957, 0.9996]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See how our model performs on the training data\n",
        "y_pred = model(x)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJng31_Pi2R6",
        "outputId": "c69fbad9-28f2-4529-c171-1849ed811a56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forward function被自动执行了\n",
            "the x2 loss is 0.008661006577312946\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.9936, 0.9991, 0.9964, 0.9900, 0.9989],\n",
              "        [0.9998, 1.0000, 0.9996, 0.9992, 1.0000],\n",
              "        [0.9218, 0.9601, 0.9449, 0.9368, 0.9573],\n",
              "        [0.9980, 0.9999, 0.9990, 0.9958, 0.9998],\n",
              "        [0.9910, 0.9975, 0.9854, 0.9865, 0.9960],\n",
              "        [0.9976, 0.9997, 0.9970, 0.9950, 0.9995],\n",
              "        [0.9941, 0.9991, 0.9956, 0.9905, 0.9988],\n",
              "        [0.9990, 0.9999, 0.9995, 0.9975, 0.9999],\n",
              "        [0.9975, 0.9995, 0.9926, 0.9945, 0.9989],\n",
              "        [0.9954, 0.9995, 0.9975, 0.9923, 0.9993]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create test data and check how our model performs on it\n",
        "x2 = y + torch.randn_like(y)\n",
        "y_pred = model(x2)\n",
        "print(\"the x2 loss is {}\".format(loss_function(y_pred, y).item()))\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WNk6oIZw2xo"
      },
      "source": [
        "Great! Looks like our model almost perfectly learned to filter out the noise from the `x` that we passed in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rUNk_1xG1v"
      },
      "source": [
        "## Demo: Word Window Classification\n",
        "\n",
        "Until this part of the notebook, we have learned the fundamentals of PyTorch and built a basic network solving a toy task. Now we will attempt to solve an example NLP task. Here are the things we will learn:\n",
        "\n",
        "1. Data: Creating a Dataset of Batched Tensors\n",
        "2. Modeling\n",
        "3. Training\n",
        "4. Prediction\n",
        "\n",
        "In this section, our goal will be to train a model that will find the words in a sentence corresponding to a `LOCATION`, which will be always of span `1` (meaning that `San Fransisco` won't be recognized as a `LOCATION`). Our task is called `Word Window Classification` for a reason. Instead of letting our model to only take a look at one word in each forward pass, we would like it to be able to consider the context of the word in question. That is, for each word, we want our model to be aware of the surrounding words. Let's dive in!\n",
        "\n",
        "到目前为止，我们已经学习了 PyTorch 的基础知识并构建了解决简单任务的基本网络模型。现在我们将尝试解决一个示例的自然语言处理（NLP）任务。以下是我们将要学习的内容：\n",
        "\n",
        "1. 数据：创建一个批量张量的数据集\n",
        "2. 建模\n",
        "3. 训练\n",
        "4. 预测\n",
        "\n",
        "在这一部分，我们的目标是训练一个模型，该模型能够找出句子中对应于 `LOCATION` 的词语，而这些词语始终只会是长度为 `1` 的词（意味着 `San Francisco` 不会被识别为 `LOCATION`）。我们的任务被称为 `Word Window Classification`，有其原因。我们希望模型在每次前向传播时不仅仅看一个单词，而是能够考虑到该单词周围的上下文。换句话说，对于每个单词，我们希望模型能够感知到其周围的其他单词。让我们开始吧！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_amzuUx8BJXI"
      },
      "source": [
        "### Data\n",
        "\n",
        "The very first task of any machine learning project is to set up our training set. Usually, there will be a training corpus we will be utilizing. In NLP tasks, the corpus would generally be a `.txt` or `.csv` file where each row corresponds to a sentence or a tabular datapoint. In our toy task, we will assume that we have already read our data and the corresponding labels into a `Python` list.\n",
        "\n",
        "任何机器学习项目的第一个任务是设置训练集。通常情况下，我们会使用一个训练语料库。在自然语言处理任务中，语料库通常是一个 .txt 或 .csv 文件，其中每一行对应一个句子或一个表格数据点。在我们的示例任务中，我们假设已经将数据及其对应的标签读入到一个 Python 列表中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "mDiI1PLMw10z"
      },
      "outputs": [],
      "source": [
        "# Our raw data, which consists of sentences\n",
        "corpus = [\n",
        "          \"We always come to Paris\",\n",
        "          \"The professor is from Australia\",\n",
        "          \"I live in Stanford\",\n",
        "          \"He comes from Taiwan\",\n",
        "          \"The capital of Turkey is Ankara\"\n",
        "         ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t33Uke9AE22s"
      },
      "source": [
        "#### Preprocessing\n",
        "\n",
        "To make it easier for our models to learn, we usually apply a few preprocessing steps to our data. This is especially important when dealing with text data. Here are some examples of text preprocessing:\n",
        "* **Tokenization**: Tokenizing the sentences into words.\n",
        "* **Lowercasing**: Changing all the letters to be lowercase.\n",
        "* **Noise removal:** Removing special characters (such as punctuations).\n",
        "* **Stop words removal**: Removing commonly used words.\n",
        "\n",
        "为了让我们的模型更容易学习，通常会对数据进行一些预处理步骤，特别是在处理文本数据时尤为重要。以下是一些文本预处理的示例：\n",
        "* **分词（Tokenization）**：将句子分割成单词。\n",
        "* **转换为小写（Lowercasing）**：将所有字母改为小写。\n",
        "* **去噪（Noise removal）**：移除特殊字符（如标点符号）。\n",
        "* **停用词去除（Stop words removal）**：移除常用词语。\n",
        "\n",
        "Which preprocessing steps are necessary is determined by the task at hand. For example, although it is useful to remove special characters in some tasks, for others they may be important (for example, if we are dealing with multiple languages). For our task, we will lowercase our words and tokenize.\n",
        "\n",
        "根据手头的任务确定哪些预处理步骤是必要的。例如，虽然在某些任务中移除特殊字符很有用，但在其他情况下它们可能很重要（例如，如果我们处理多种语言）。对于我们的任务，我们将会将单词转换为小写并进行分词。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTGn8ANTzZXT",
        "outputId": "5ac3bc25-e5dd-4005-aa36-90836345fb15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['we', 'always', 'come', 'to', 'paris'],\n",
              " ['the', 'professor', 'is', 'from', 'australia'],\n",
              " ['i', 'live', 'in', 'stanford'],\n",
              " ['he', 'comes', 'from', 'taiwan'],\n",
              " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The preprocessing function we will use to generate our training examples\n",
        "# Our function is a simple one, we lowercase the letters\n",
        "# and then tokenize the words.\n",
        "def preprocess_sentence(sentence):\n",
        "  return sentence.lower().split()\n",
        "\n",
        "# Create our training set\n",
        "train_sentences = [sent.lower().split() for sent in corpus]\n",
        "train_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4jzo5tp0Hza"
      },
      "source": [
        "For each training example we have, we should also have a corresponding label. Recall that the goal of our model was to determine which words correspond to a `LOCATION`. That is, we want our model to output `0` for all the words that are not `LOCATION`s and `1` for the ones that are `LOCATION`s.\n",
        "\n",
        "对于每个训练样本，我们都应该有一个相应的标签。回想一下，我们模型的目标是确定哪些单词对应于“地点”。也就是说，我们希望模型对所有不是“地点”的单词输出`0`，对那些是“地点”的单词输出`1`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wo1kcMAHFw7",
        "outputId": "ffef4ec0-d693-4137-ba40-f9112db5ea55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 1],\n",
              " [0, 0, 0, 1],\n",
              " [0, 0, 0, 1, 0, 1]]"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set of locations that appear in our corpus\n",
        "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
        "\n",
        "# Our train labels\n",
        "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgVKH9M3RtPx"
      },
      "source": [
        "#### Converting Words to Embeddings\n",
        "\n",
        "Let's look at our training data a little more closely. Each datapoint we have is a sequence of words. On the other hand, we know that machine learning models work with numbers in vectors. How are we going to turn words into numbers? You may be thinking embeddings and you are right!\n",
        "\n",
        "让我们更仔细地看看我们的训练数据。我们每个数据点都是一个单词序列。另一方面，我们知道机器学习模型处理的是向量中的数字。我们要如何将单词转换成数字呢？你可能会想到嵌入，你是对的！\n",
        "\n",
        "Imagine that we have an embedding lookup table `E`, where each row corresponds to an embedding. That is, each word in our vocabulary would have a corresponding embedding row `i` in this table. Whenever we want to find an embedding for a word, we will follow these steps:\n",
        "1. Find the corresponding index `i` of the word in the embedding table: `word->index`.\n",
        "2. Index into the embedding table and get the embedding: `index->embedding`.\n",
        "\n",
        "想象我们有一个嵌入查找表 `E`，其中每一行对应一个嵌入。也就是说，我们词汇表中的每个单词在这个表中都有一个对应的嵌入行 `i`。每当我们想要找到一个单词的嵌入时，我们将按照以下步骤进行：\n",
        "1. 找到该单词在嵌入表中的对应索引 `i`：`单词->索引`。\n",
        "2. 在嵌入表中索引并获取嵌入：`索引->嵌入`。\n",
        "\n",
        "Let's look at the first step. We should assign all the words in our vocabulary to a corresponding index. We can do it as follows:\n",
        "1. Find all the unique words in our corpus.\n",
        "2. Assign an index to each.\n",
        "\n",
        "让我们看看第一步。我们应该将词汇表中的所有单词分配一个对应的索引。我们可以按以下步骤进行：\n",
        "1. 找出我们语料库中的所有唯一单词。\n",
        "2. 给每个单词分配一个索引。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjTDlfPyVp5z",
        "outputId": "9f36d85f-bc74-4d45-ef79-def80b5a8f6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'always',\n",
              " 'ankara',\n",
              " 'australia',\n",
              " 'capital',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'from',\n",
              " 'he',\n",
              " 'i',\n",
              " 'in',\n",
              " 'is',\n",
              " 'live',\n",
              " 'of',\n",
              " 'paris',\n",
              " 'professor',\n",
              " 'stanford',\n",
              " 'taiwan',\n",
              " 'the',\n",
              " 'to',\n",
              " 'turkey',\n",
              " 'we'}"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Find all the unique words in our corpus\n",
        "vocabulary = set(w for s in train_sentences for w in s)\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOxnKznOWXSC"
      },
      "source": [
        "`vocabulary` now contains all the words in our corpus. On the other hand, during the test time, we can see words that are not contained in our vocabulary. If we can figure out a way to represent the unknown words, our model can still reason about whether they are a `LOCATION` or not, since we are also looking at the neighboring words for each prediction.\n",
        "\n",
        "`vocabulary` 现在包含了我们语料库中的所有单词。另一方面，在测试时，我们可能会看到不在词汇表中的单词。如果我们能找到一种方法来表示未知单词，我们的模型仍然可以推断它们是否是 `LOCATION`，因为在每次预测时我们也会查看邻近的单词。\n",
        "\n",
        "We introduce a special token, `<unk>`, to tackle the words that are out of vocabulary. We could pick another string for our unknown token if we wanted. The only requirement here is that our token should be unique: we should only be using this token for unknown words. We will also add this special token to our vocabulary.\n",
        "\n",
        "我们引入一个特殊的标记 `<unk>` 来处理超出词汇表的单词。如果愿意，我们可以选择另一个字符串作为我们的未知标记。唯一的要求是我们的标记应该是唯一的：我们只应使用这个标记来表示未知单词。我们还会将这个特殊标记添加到我们的词汇表中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "ygxYuE1DYeR3"
      },
      "outputs": [],
      "source": [
        "# Add the unknown token to our vocabulary\n",
        "vocabulary.add(\"<unk>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsf4haL94AFu"
      },
      "source": [
        "Earlier we mentioned that our task was called `Word Window Classification` because our model is looking at the surroundings words in addition to the given word when it needs to make a prediction.\n",
        "\n",
        "前面我们提到过，我们的任务被称为 `Word Window Classification`，因为在模型需要进行预测时，它不仅会查看给定的单词，还会查看周围的单词。\n",
        "\n",
        "For example, let's take the sentence \"We always come to Paris\". The corresponding training label for this sentence is `0, 0, 0, 0, 1` since only Paris, the last word, is a `LOCATION`. In one pass (meaning a call to `forward()`), our model will try to generate the correct label for one word. Let's say our model is trying to generate the correct label `1` for `Paris`. If we only allow our model to see `Paris`, but nothing else, we will miss out on the important information that the word `to` often times appears with `LOCATION`s.\n",
        "\n",
        "例如，假设句子是 \"We always come to Paris\"。由于只有最后一个单词 Paris 是一个 `LOCATION`，因此该句子的对应训练标签是 `0, 0, 0, 0, 1`。在一次传递（即一次调用 `forward()`）中，我们的模型将尝试为一个单词生成正确的标签。假设我们的模型试图为 `Paris` 生成正确的标签 `1`。如果我们只允许模型看到 `Paris`，而看不到其他内容，我们将错过“to”这个单词通常与 `LOCATION` 一起出现的重要信息。\n",
        "\n",
        "Word windows allow our model to consider the surrounding `+N` or `-N` words of each word when making a prediction. In our earlier example for `Paris`, if we have a window size of 1, that means our model will look at the words that come immediately before and after `Paris`, which are `to`, and, well, nothing. Now, this raises another issue. `Paris` is at the end of our sentence, so there isn't another word following it. Remember that we define the input dimensions of our `PyTorch` models when we are initializing them. If we set the window size to be `1`, it means that our model will be accepting `3` words in every pass. We cannot have our model expect `2` words from time to time.\n",
        "\n",
        "单词窗口允许我们的模型在做出预测时考虑每个单词周围的 `+N` 或 `-N` 个单词。在前面关于 `Paris` 的例子中，如果我们有一个窗口大小为 1，这意味着我们的模型将查看紧接在 `Paris` 之前和之后的单词，即 `to` 和，嗯，没有其他单词。这引发了另一个问题。`Paris` 在句子的末尾，所以没有另一个单词跟在它后面。记住，我们在初始化 `PyTorch` 模型时定义了输入维度。如果我们将窗口大小设置为 `1`，这意味着我们的模型将在每次传递中接受 `3` 个单词。我们不能让模型有时只期望 `2` 个单词。\n",
        "\n",
        "The solution is to introduce a special token, such as `<pad>`, that will be added to our sentences to make sure that every word has a valid window around them. Similar to `<unk>` token, we could pick another string for our pad token if we wanted, as long as we make sure it is used for a unique purpose.\n",
        "\n",
        "解决方案是引入一个特殊的标记，比如 `<pad>`，将其添加到我们的句子中，以确保每个单词周围都有一个有效的窗口。类似于 `<unk>` 标记，如果愿意，我们可以为我们的填充标记选择另一个字符串，只要确保它用于唯一的目的即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVQsjYi6ZegI",
        "outputId": "7833a42b-0f2c-4b6a-bda1-3b06503983eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add the <pad> token to our vocabulary\n",
        "vocabulary.add(\"<pad>\")\n",
        "\n",
        "# Function that pads the given sentence\n",
        "# We are introducing this function here as an example\n",
        "# We will be utilizing it later in the tutorial\n",
        "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
        "  window = [pad_token] * window_size\n",
        "  return window + sentence + window\n",
        "\n",
        "# Show padding example\n",
        "window_size = 2\n",
        "pad_window(train_sentences[0], window_size=window_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqvgWKwSNpAd"
      },
      "source": [
        "Now that our vocabularly is ready, let's assign an index to each of our words.\n",
        "\n",
        "现在我们的词汇表准备好了，让我们为每个单词分配一个索引。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNCTQnKDa4oh",
        "outputId": "54033f5b-e96d-47b0-f93c-bbc12a84bdbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<pad>': 0,\n",
              " '<unk>': 1,\n",
              " 'always': 2,\n",
              " 'ankara': 3,\n",
              " 'australia': 4,\n",
              " 'capital': 5,\n",
              " 'come': 6,\n",
              " 'comes': 7,\n",
              " 'from': 8,\n",
              " 'he': 9,\n",
              " 'i': 10,\n",
              " 'in': 11,\n",
              " 'is': 12,\n",
              " 'live': 13,\n",
              " 'of': 14,\n",
              " 'paris': 15,\n",
              " 'professor': 16,\n",
              " 'stanford': 17,\n",
              " 'taiwan': 18,\n",
              " 'the': 19,\n",
              " 'to': 20,\n",
              " 'turkey': 21,\n",
              " 'we': 22}"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We are just converting our vocabularly to a list to be able to index into it\n",
        "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
        "# That being said, we will see that having the index for the padding token\n",
        "# be 0 is convenient as some PyTorch functions use it as a default value\n",
        "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
        "ix_to_word = sorted(list(vocabulary))\n",
        "\n",
        "# Creating a dictionary to find the index of a given word\n",
        "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
        "word_to_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pt-0SK67hMVo",
        "outputId": "a914da1b-6e1c-4309-acd5-bd4a53b7c2b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<unk>'"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ix_to_word[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELZuteqbdWd1"
      },
      "source": [
        "Great! We are ready to convert our training sentences into a sequence of indices corresponding to each token.\n",
        "\n",
        "太好了！我们已经准备好将我们的训练句子转换为每个标记对应的索引序列了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNOxip15bMfH",
        "outputId": "01bba9e6-651a-47e8-a3d7-cf5b89ca919d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
            "Going from words to indices: [22, 2, 6, 20, 1]\n",
            "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n",
            "the compacted function has same output: [22, 2, 6, 20, 1]\n"
          ]
        }
      ],
      "source": [
        "# Given a sentence of tokens, return the corresponding indices\n",
        "def convert_token_to_indices(sentence, word_to_ix):\n",
        "  indices = []\n",
        "  for token in sentence:\n",
        "    # Check if the token is in our vocabularly. If it is, get it's index.\n",
        "    # If not, get the index for the unknown token.\n",
        "    if token in word_to_ix:\n",
        "      index = word_to_ix[token]\n",
        "    else:\n",
        "      index = word_to_ix[\"<unk>\"]\n",
        "    indices.append(index)\n",
        "  return indices\n",
        "\n",
        "# More compact version of the same function\n",
        "# 完成了和上面函数一样的功能！\n",
        "def _convert_token_to_indices(sentence, word_to_ix):\n",
        "  # return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
        "  # dict的get()方法尝试从 word_to_ix 字典中获取 token（即单词）的索引\n",
        "  return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
        "\n",
        "# Show an example\n",
        "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
        "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
        "example_indices_test = _convert_token_to_indices(example_sentence, word_to_ix)\n",
        "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
        "\n",
        "print(f\"Original sentence is: {example_sentence}\")\n",
        "print(f\"Going from words to indices: {example_indices}\")\n",
        "print(f\"Going from indices to words: {restored_example}\")\n",
        "print(f\"the compacted function has same output: {example_indices_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jsXw8cB1xpH"
      },
      "source": [
        "In the example above, `kuwait` shows up as `<unk>`, because it is not included in our vocabulary. Let's convert our `train_sentences` to `example_padded_indices`.\n",
        "\n",
        "在上面的例子中，`kuwait` 显示为 `<unk>`，因为它未包含在我们的词汇表中。让我们将我们的 `train_sentences` 转换为 `example_padded_indices`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRaKQwSJH-1d",
        "outputId": "7ac54f70-e1ee-4418-a803-42c9ee873074"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[22, 2, 6, 20, 15],\n",
              " [19, 16, 12, 8, 4],\n",
              " [10, 13, 11, 17],\n",
              " [9, 7, 8, 18],\n",
              " [19, 5, 14, 21, 12, 3]]"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Converting our sentences to indices\n",
        "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
        "example_padded_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZULjHBjHsEK"
      },
      "source": [
        "Now that we have an index for each word in our vocabularly, we can create an embedding table with `nn.Embedding` class in `PyTorch`. It is called as follows `nn.Embedding(num_words, embedding_dimension)` where `num_words` is the number of words in our vocabulary and the `embedding_dimension` is the dimension of the embeddings we want to have. There is nothing fancy about `nn.Embedding`: it is just a wrapper class around a trainabe `NxE` dimensional tensor, where `N` is the number of words in our vocabulary and `E` is the number of embedding dimensions. This table is initially random, but it will change over time. As we train our network, the gradients will be backpropagated all the way to the embedding layer, and hence our word embeddings would be updated. We will initiliaze the embedding layer we will use for our model in our model, but we are showing an example here.\n",
        "\n",
        "现在我们为词汇表中的每个单词都有了一个索引，我们可以使用 `PyTorch` 中的 `nn.Embedding` 类创建一个嵌入表。它的调用方式如下：`nn.Embedding(num_words, embedding_dimension)`，其中 `num_words` 是我们词汇表中的单词数量，`embedding_dimension` 是我们希望拥有的嵌入维度。关于 `nn.Embedding` 并没有什么特别的：它只是一个围绕可训练的 `NxE` 维张量的包装类，其中 `N` 是词汇表中单词的数量，`E` 是嵌入维度。这个表最初是随机的，但会随着时间改变。当我们训练网络时，梯度将一直反向传播到嵌入层，因此我们的词嵌入会得到更新。我们将在模型中初始化用于嵌入层的嵌入表，这里只是展示一个示例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4AgHzv91VXx",
        "outputId": "c718e46d-af30-459a-ed12-510926044d03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-8.8797e-01,  9.0129e-02,  4.9432e-01, -5.7977e-02, -2.4530e-01],\n",
              "         [ 7.2132e-01, -9.1917e-01,  5.2815e-04, -2.8196e-01,  1.6774e+00],\n",
              "         [ 1.1514e-02, -8.3316e-02,  1.1639e+00, -1.4331e-01, -1.4484e+00],\n",
              "         [-1.5556e+00, -7.0344e-01, -7.2735e-01,  3.9447e-02, -2.9022e-01],\n",
              "         [-1.4843e+00, -8.6632e-02, -2.0215e+00, -1.0391e-01,  2.5616e-01],\n",
              "         [ 6.4172e-01,  2.9437e-01, -5.7398e-01,  2.0036e+00, -4.8637e-01],\n",
              "         [-9.3919e-01,  1.1492e-01, -2.0537e-01,  5.4826e-01, -1.3616e-01],\n",
              "         [-4.4233e-02, -8.7528e-01,  1.6569e+00, -4.6451e-01, -6.2155e-01],\n",
              "         [ 3.1305e-01, -1.5734e+00,  2.0768e-01,  7.9895e-01, -5.2259e-02],\n",
              "         [ 1.1877e-03,  1.0866e+00, -1.0282e+00,  9.7133e-01, -1.8694e+00],\n",
              "         [-8.0738e-01,  7.7805e-01, -4.0332e-01,  1.0427e+00,  1.1000e+00],\n",
              "         [ 3.6213e-01, -1.3958e-02,  1.2187e+00, -1.0414e+00, -7.5985e-02],\n",
              "         [-9.9113e-01, -1.3071e+00,  2.4603e+00, -2.1908e+00, -1.5474e+00],\n",
              "         [ 1.6776e-01, -1.7546e+00, -3.0681e-01,  6.7266e-01,  5.4509e-01],\n",
              "         [ 7.3276e-01, -1.5430e-01,  1.2675e+00,  9.6053e-01,  1.0325e+00],\n",
              "         [-4.3720e-02, -6.2413e-01,  5.7526e-02, -1.1279e+00,  8.4859e-02],\n",
              "         [ 1.1682e-01,  2.0599e-01,  5.7830e-02, -3.3052e-01, -6.7591e-01],\n",
              "         [ 2.8089e+00, -1.9633e-01,  7.2497e-01,  1.1665e+00,  9.1127e-01],\n",
              "         [-1.1800e+00,  3.0249e-01, -1.2733e+00, -5.2948e-01,  3.4461e-01],\n",
              "         [ 1.4504e-01,  2.1176e-01, -1.5474e+00, -2.8272e-01,  3.2351e-01],\n",
              "         [ 1.5513e-01, -9.4937e-01,  1.6017e+00,  2.9901e-01, -3.6827e-01],\n",
              "         [-1.9582e-01, -4.6218e-01, -6.4274e-02, -1.0662e+00, -5.7416e-02],\n",
              "         [ 5.7697e-01, -1.9516e-01, -5.4322e-01,  3.8652e-03, -1.6349e-01]],\n",
              "        requires_grad=True)]"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating an embedding table for our words\n",
        "embedding_dim = 5\n",
        "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
        "\n",
        "# Printing the parameters in our embedding table\n",
        "list(embeds.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI7ZTt4OkpPp"
      },
      "source": [
        "To get the word embedding for a word in our vocabulary, all we need to do is to create a lookup tensor. The lookup tensor is just a tensor containing the index we want to look up `nn.Embedding` class expects an index tensor that is of type Long Tensor, so we should create our tensor accordingly.\n",
        "\n",
        "要获取我们词汇表中单词的词嵌入，我们只需要创建一个查找张量。查找张量只是一个包含我们想要查找的索引的张量。`nn.Embedding` 类期望一个长整型张量作为索引张量，因此我们应该相应地创建我们的张量。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在 PyTorch 中，nn.Embedding 类实例化后，其对象 embeds 是一个可以调用的对象，具有一个 __call__ 方法，这使得它能够像函数一样被调用。当你使用 embeds(index_tensor) 时，实际上调用了 __call__ 方法，该方法接受一个索引张量 index_tensor 作为输入，并返回对应索引处的嵌入向量。\n",
        "\n",
        "具体来说，nn.Embedding 内部维护了一个嵌入表（embedding table），这是一个可学习的参数矩阵，其形状为 (num_embeddings, embedding_dim)，其中 num_embeddings 是词汇表的大小，embedding_dim 是每个嵌入向量的维度。当你调用 embeds(index_tensor) 时，PyTorch 会自动根据 index_tensor 中的每个索引，从嵌入表中获取对应的嵌入向量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkldmcepjfh_",
        "outputId": "278c2d7e-acf3-426d-ea29-10f85cf11277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(15)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([-0.0437, -0.6241,  0.0575, -1.1279,  0.0849],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the embedding for the word Paris\n",
        "# 随便初始化了一个embeds，具有vocabulary的形状大小，只要后面不断用他去处理vocabulary\n",
        "# 那他就相当于是vocabulary的专用embeds了\n",
        "index = word_to_ix[\"paris\"]\n",
        "index_tensor = torch.tensor(index, dtype=torch.long)\n",
        "print(index_tensor)\n",
        "# 使用 index_tensor 查找并获取 \"paris\" 的嵌入向量\n",
        "paris_embed = embeds(index_tensor)\n",
        "paris_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUsdwBOxm6B4",
        "outputId": "8c4ced79-7235-43f5-ec59-cd33df1d9ca9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0437, -0.6241,  0.0575, -1.1279,  0.0849],\n",
              "        [-1.5556, -0.7034, -0.7274,  0.0394, -0.2902]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also get multiple embeddings at once\n",
        "index_paris = word_to_ix[\"paris\"]\n",
        "index_ankara = word_to_ix[\"ankara\"]\n",
        "indices = [index_paris, index_ankara]\n",
        "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
        "embeddings = embeds(indices_tensor)\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bgSW3LPltkF"
      },
      "source": [
        "Usually, we define the embedding layer as part of our model, which you will see in the later sections of our notebook.\n",
        "\n",
        "通常，我们将嵌入层定义为模型的一部分，这一点将在我们笔记本的后续部分中看到。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHCXeQOamHU1"
      },
      "source": [
        "#### Batching Sentences 批处理句子\n",
        "\n",
        "We have learned about batches in class. Waiting our whole training corpus to be processed before making an update is constly. On the other hand, updating the parameters after every training example causes the loss to be less stable between updates. To combat these issues, we instead update our parameters after training on a batch of data. This allows us to get a better estimate of the gradient of the global loss. In this section, we will learn how to structure our data into batches using the `torch.util.data.DataLoader` class.\n",
        "\n",
        "我们在课堂上学习过批处理的概念。等待整个训练语料库被处理完毕再进行更新是昂贵的。另一方面，每处理一个训练样本后就更新参数会导致更新之间的损失不稳定。为了应对这些问题，我们改为在一批数据上训练后再更新参数。这样可以更好地估计全局损失的梯度。在本节中，我们将学习如何使用 `torch.util.data.DataLoader` 类将数据分成批次。\n",
        "\n",
        "We will be calling the `DataLoader` class as follows: `DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`.  The `batch_size` parameter determines the number of examples per batch. In every epoch, we will be iterating over all the batches using the `DataLoader`. The order of batches is deterministic by default, but we can ask `DataLoader` to shuffle the batches by setting the `shuffle` parameter to `True`. This way we ensure that we don't encounter a bad batch multiple times.\n",
        "\n",
        "我们将如下调用 `DataLoader` 类：`DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)`。`batch_size` 参数确定每个批次的示例数。每个 epoch 中，我们将使用 `DataLoader` 迭代所有批次。默认情况下，批次的顺序是确定性的，但我们可以通过将 `shuffle` 参数设置为 `True`，要求 `DataLoader` 对批次进行洗牌。这样可以确保我们不会多次遇到同一个不良批次。\n",
        "\n",
        "将 `shuffle` 参数设置为 `True`，在每个 epoch 开始时，数据集中的数据会被随机打乱，因此组成的每个batch的搭配情况也会不一样。\n",
        "\n",
        "If provided, `DataLoader` passes the batches it prepares to the `collate_fn`. We can write a custom function to pass to the `collate_fn` parameter in order to print stats about our batch or perform extra processing. In our case, we will use the `collate_fn` to:\n",
        "1. Window pad our train sentences.\n",
        "2. Convert the words in the training examples to indices.\n",
        "3. Pad the training examples so that all the sentences and labels have the same length. Similarly, we also need to pad the labels. This creates an issue because when calculating the loss, we need to know the actual number of words in a given example. We will also keep track of this number in the function we pass to the `collate_fn` parameter.\n",
        "\n",
        "如果提供了，`DataLoader` 将准备好的批次传递给 `collate_fn`。我们可以编写一个自定义函数，将其传递给 `collate_fn` 参数，以便打印有关我们批次的统计信息或执行额外的处理。在我们的情况下，我们将使用 `collate_fn` 来完成以下任务：\n",
        "1. 对训练句子进行窗口填充。\n",
        "2. 将训练示例中的单词转换为索引。\n",
        "3. 对训练示例进行填充，以使所有句子和标签具有相同的长度。同样，我们还需要对标签进行填充。这会导致一个问题，因为在计算损失时，我们需要知道给定示例中实际的单词数。我们还将在传递给 `collate_fn` 参数的函数中跟踪这个数量。\n",
        "\n",
        "Because our version of the `collate_fn` function will need to access to our `word_to_ix` dictionary (so that it can turn words into indices), we will make use of the `partial` function in `Python`, which passes the parameters we give to the function we pass it.\n",
        "\n",
        "因为我们的 `collate_fn` 函数版本需要访问我们的 `word_to_ix` 字典（以便将单词转换为索引），我们将利用 `Python` 中的 `partial` 函数，该函数将我们提供的参数传递给我们传递给它的函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "OkvvVlo4jgFm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "\n",
        "def custom_collate_fn(batch, window_size, word_to_ix):\n",
        "  # Break our batch into the training examples (x) and labels (y)\n",
        "  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n",
        "  # method expects tensors. This is also useful since our model will be\n",
        "  # expecting tensor inputs.\n",
        "  # 使用了“解包”操作符*，这意味着它将 batch 列表中的每个元素作为单独的参数传递给 zip 函数\n",
        "  # zip 函数会将多个迭代器的对应元素打包成一个个元组，然后返回这些元组组成的迭代器\n",
        "  # 所以这里x, y会得到两个对应的元组\n",
        "  x, y = zip(*batch)\n",
        "\n",
        "  # Now we need to window pad our training examples. We have already defined a\n",
        "  # function to handle window padding. We are including it here again so that\n",
        "  # everything is in one place.\n",
        "  # 为window增加pad，避免前后元素不满足window_size的检查范围\n",
        "  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
        "    window = [pad_token] * window_size\n",
        "    return window + sentence + window\n",
        "\n",
        "  # Pad the train examples.\n",
        "  x = [pad_window(s, window_size=window_size) for s in x]\n",
        "\n",
        "  # Now we need to turn words in our training examples to indices. We are\n",
        "  # copying the function defined earlier for the same reason as above.\n",
        "  # 现在我们需要将训练示例中的单词转换为索引。我们复制之前定义的函数，原因与上述相同。\n",
        "  def convert_tokens_to_indices(sentence, word_to_ix):\n",
        "    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
        "\n",
        "  # Convert the train examples into indices.\n",
        "  # dict的get()方法尝试从 word_to_ix 字典中获取 token（即单词）的索引\n",
        "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
        "\n",
        "  # We will now pad the examples so that the lengths of all the example in\n",
        "  # one batch are the same, making it possible to do matrix operations.\n",
        "  # We set the batch_first parameter to True so that the returned matrix has\n",
        "  # the batch as the first dimension.\n",
        "  # 我们现在将对示例进行填充，使一个批次中所有示例的长度相同，从而可以进行矩阵运算。\n",
        "  # 我们将 batch_first 参数设置为 True，以便返回的矩阵将批次作为第一个维度。\n",
        "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
        "\n",
        "  # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
        "  # 填充序列：将一批序列（张量列表）填充到相同的长度，以便于后续的批处理操作\n",
        "  # 相关参数：\n",
        "  # sequences：要填充的序列列表，每个序列是一个张量。\n",
        "  # batch_first：布尔值，指定返回的张量是否以批次维度为第一维度，默认为 False。如果设置为 True，则返回的张量形状为 (batch_size, max_length, *)；否则为 (max_length, batch_size, *)。\n",
        "  # padding_value：用于填充的值，默认为 0。\n",
        "  x = [torch.LongTensor(x_i) for x_i in x]\n",
        "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
        "\n",
        "  # We will also pad the labels. Before doing so, we will record the number\n",
        "  # of labels so that we know how many words existed in each example.\n",
        "  lengths = [len(label) for label in y]\n",
        "  lenghts = torch.LongTensor(lengths)\n",
        "\n",
        "  y = [torch.LongTensor(y_i) for y_i in y]\n",
        "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "\n",
        "  # We are now ready to return our variables. The order we return our variables\n",
        "  # here will match the order we read them in our training loop.\n",
        "  return x_padded, y_padded, lenghts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q0gXea-bCsz"
      },
      "source": [
        "This function seems long, but it really doesn't have to be. Check out the alternative version below where we remove the extra function declarations and comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "dZfcmAJXbLcq"
      },
      "outputs": [],
      "source": [
        "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
        "  # Prepare the datapoints\n",
        "  x, y = zip(*batch)\n",
        "  # 加pad字符放置window超出\n",
        "  # 字符变数字,以便运算\n",
        "  x = [pad_window(s, window_size=window_size) for s in x]\n",
        "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
        "\n",
        "  # Pad x so that all the examples in the batch have the same size\n",
        "  # 进行pad,变成矩阵\n",
        "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
        "  x = [torch.LongTensor(x_i) for x_i in x]\n",
        "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
        "\n",
        "  # Pad y and record the length\n",
        "  # 对标签y进行pad,变成矩阵\n",
        "  lengths = [len(label) for label in y]\n",
        "  lenghts = torch.LongTensor(lengths)\n",
        "  y = [torch.LongTensor(y_i) for y_i in y]\n",
        "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "\n",
        "  return x_padded, y_padded, lenghts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS1WuQO0Khxx"
      },
      "source": [
        "Now, we can see the `DataLoader` in action.\n",
        "\n",
        "zip() 函数可以将多个序列按顺序打包成元组，这些元组可以同时迭代"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "collate_fn 返回了三个参数：batched_x（处理后的特征数据）、batched_y（处理后的标签数据）、batched_lengths（样本长度信息）。这种灵活性使得 DataLoader 能够适应不同类型和形状的数据，同时也支持处理不同形式的输入和输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfB0JKL2vZ6p",
        "outputId": "1d689d9e-05fb-4f45-9fb8-9363af241131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Batched Input:\n",
            "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0],\n",
            "        [ 0,  0, 10, 13, 11, 17,  0,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 1, 0]])\n",
            "Batched Lengths:\n",
            "tensor([5, 4])\n",
            "\n",
            "Iteration 1\n",
            "Batched Input:\n",
            "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0],\n",
            "        [ 0,  0, 19, 16, 12,  8,  4,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 1]])\n",
            "Batched Lengths:\n",
            "tensor([4, 5])\n",
            "\n",
            "Iteration 2\n",
            "Batched Input:\n",
            "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
            "Batched Labels:\n",
            "tensor([[0, 0, 0, 1, 0, 1]])\n",
            "Batched Lengths:\n",
            "tensor([6])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Parameters to be passed to the DataLoader\n",
        "data = list(zip(train_sentences, train_labels))\n",
        "batch_size = 2\n",
        "shuffle = True\n",
        "window_size = 2\n",
        "# partial是部分应用\n",
        "# 部分应用是指固定一个函数的一部分参数，然后生成一个新的函数，该新函数接受剩余的参数\n",
        "# collate_fn可以输入内容并分割数据和标签, 调整成为矩阵格式用于训练\n",
        "# 也就是数据预处理\n",
        "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
        "\n",
        "# Instantiate the DataLoader\n",
        "# DataLoader 可以将数据集分成批次（batch），每个批次包含指定数量的数据样本。这样做有助于提高训练效率，特别是在大型数据集上。\n",
        "# collate_fn：用于自定义批处理过程的函数，默认为 None。\n",
        "# 如果提供了 collate_fn，DataLoader 在每个批次加载数据之前会调用此函数对数据进行预处理，如填充、转换等操作。\n",
        "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "\n",
        "# Go through one loop\n",
        "counter = 0\n",
        "# 由于loader中的collate_fn能返回三个参数，所以对应了迭代器中的参数数量\n",
        "for batched_x, batched_y, batched_lengths in loader:\n",
        "  print(f\"Iteration {counter}\")\n",
        "  print(\"Batched Input:\")\n",
        "  print(batched_x)\n",
        "  print(\"Batched Labels:\")\n",
        "  print(batched_y)\n",
        "  print(\"Batched Lengths:\")\n",
        "  print(batched_lengths)\n",
        "  print(\"\")\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93QOZSsMTFNF"
      },
      "source": [
        "The batched input tensors you see above will be passed into our model. On the other hand, we started off saying that our model will be a window classifier. The way our input tensors are currently formatted, we have all the words in a sentence in one datapoint. When we pass this input to our model, it needs to create the windows for each word, make a prediction as to whether the center word is a `LOCATION` or not for each window, put the predictions together and return.\n",
        "\n",
        "上面你看到的批处理输入张量将被传递到我们的模型中。另一方面，我们开始时说过，我们的模型将是一个窗口分类器。目前我们的输入张量格式化后，每个数据点中包含了一个句子中的所有单词。当我们将这个输入传递给我们的模型时，它需要为每个单词创建窗口，针对每个窗口预测中心单词是否是“LOCATION”，然后将这些预测组合在一起并返回结果。\n",
        "\n",
        "We could avoid this problem if we formatted our data by breaking it into windows beforehand. In this example, we will instead how our model take care of the formatting.\n",
        "\n",
        "我们可以避免这个问题，如果我们在预先将数据分解为窗口的情况下进行格式化。在这个例子中，我们将展示如何让我们的模型处理这种格式化。\n",
        "\n",
        "Given that our `window_size` is `N` we want our model to make a prediction on every `2N+1` tokens. That is, if we have an input with `9` tokens, and a `window_size` of `2`, we want our model to return `5` predictions. This makes sense because before we padded it with `2` tokens on each side, our input also had `5` tokens in it!\n",
        "\n",
        "假设我们的 `window_size` 是 `N`，我们希望我们的模型在每 `2N+1` 个标记上进行预测。也就是说，如果我们有一个包含 `9` 个标记的输入，窗口大小为 `2`，我们希望我们的模型返回 `5` 个预测结果。这是有道理的，因为在我们在每一侧填充 `2` 个标记之前，我们的输入中也有 `5` 个标记！\n",
        "\n",
        "We can create these windows by using for loops, but there is a faster `PyTorch` alternative, which is the `unfold(dimension, size, step)` method. We can create the windows we need using this method as follows:\n",
        "\n",
        "我们可以使用循环来创建这些窗口，但是在 PyTorch 中有一个更快的替代方法，那就是 `unfold(dimension, size, step)` 方法。我们可以按照以下方式使用这个方法来创建所需的窗口：\n",
        "\n",
        "`unfold(dimension, size, step)`方法：\n",
        "\n",
        "dimension：指定在哪个维度上创建滑动窗口。例如，对于一个二维张量（如图片数据），可以选择在行（0）或列（1）上进行滑动窗口操作。\n",
        "\n",
        "size：窗口的大小，即每个滑动窗口的长度。在给定的维度上，窗口的大小决定了滑动窗口的片段长度。\n",
        "\n",
        "step：窗口在指定维度上滑动的步长。默认为 1，表示每次滑动一个元素；可以设置为大于 1 的整数，以实现更大的步长。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMZu-pxLVxHQ",
        "outputId": "bf7467d5-632d-4c0a-8bf2-91a24e834f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Tensor: \n",
            "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
            "\n",
            "Windows: \n",
            "tensor([[[ 0,  0, 19,  5, 14],\n",
            "         [ 0, 19,  5, 14, 21],\n",
            "         [19,  5, 14, 21, 12],\n",
            "         [ 5, 14, 21, 12,  3],\n",
            "         [14, 21, 12,  3,  0],\n",
            "         [21, 12,  3,  0,  0]]])\n"
          ]
        }
      ],
      "source": [
        "# Print the original tensor\n",
        "print(f\"Original Tensor: \")\n",
        "print(batched_x)\n",
        "print(\"\")\n",
        "\n",
        "# Create the 2 * 2 + 1 chunks\n",
        "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
        "print(f\"Windows: \")\n",
        "print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "实操理解一下unfold函数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1, 4],\n",
            "         [2, 5],\n",
            "         [3, 6]],\n",
            "\n",
            "        [[4, 7],\n",
            "         [5, 8],\n",
            "         [6, 9]]])\n",
            "tensor([[[1, 2],\n",
            "         [2, 3]],\n",
            "\n",
            "        [[4, 5],\n",
            "         [5, 6]],\n",
            "\n",
            "        [[7, 8],\n",
            "         [8, 9]]])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6],\n",
        "                       [7, 8, 9]])\n",
        "\n",
        "tensor_try_window_a = tensor.unfold(0, 2, 1)\n",
        "tensor_try_window_b = tensor.unfold(1, 2, 1)\n",
        "print(tensor_try_window_a)\n",
        "print(tensor_try_window_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlDbOpeoSKxd"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now that we have prepared our data, we are ready to build our model. We have learned how to write custom `nn.Module` classes. We will do the same here and put everything we have learned so far together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "JLTU4h76NLYm"
      },
      "outputs": [],
      "source": [
        "class WordWindowClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
        "    super(WordWindowClassifier, self).__init__()\n",
        "\n",
        "    \"\"\" Instance variables \"\"\"\n",
        "    self.window_size = hyperparameters[\"window_size\"]\n",
        "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
        "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
        "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
        "\n",
        "    \"\"\" Embedding Layer\n",
        "    Takes in a tensor containing embedding indices, and returns the\n",
        "    corresponding embeddings. The output is of dim\n",
        "    (number_of_indices * embedding_dim).\n",
        "\n",
        "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
        "    non-trainable. This is useful if we only want the parameters other than the\n",
        "    embeddings parameters to change.\n",
        "\n",
        "    \"\"\"\n",
        "    # nn.Embedding 是 PyTorch 中的一个类，用于实现词嵌入（Word Embedding）的功能。\n",
        "    # 词嵌入是将单词映射到连续向量空间中的表示形式，通常用于自然语言处理任务中，如文本分类、命名实体识别等。\n",
        "    # vocab_size是词汇表大小\n",
        "    # self.embed_dim是词嵌入向量的维度，即每个单词将被映射到的向量空间的维度大小\n",
        "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
        "    if self.freeze_embeddings:\n",
        "      self.embed_layer.weight.requires_grad = False\n",
        "\n",
        "    \"\"\" Hidden Layer\n",
        "    \"\"\"\n",
        "    full_window_size = 2 * window_size + 1\n",
        "    self.hidden_layer = nn.Sequential(\n",
        "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim),\n",
        "      nn.Tanh()\n",
        "    )\n",
        "\n",
        "    \"\"\" Output Layer\n",
        "    \"\"\"\n",
        "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
        "\n",
        "    \"\"\" Probabilities\n",
        "    \"\"\"\n",
        "    self.probabilities = nn.Sigmoid()\n",
        "\n",
        "  # 模型在init里面定义的层，在forward中进行具体的使用！\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Let B:= batch_size\n",
        "        L:= window-padded sentence length\n",
        "        D:= self.embed_dim\n",
        "        S:= self.window_size\n",
        "        H:= self.hidden_dim\n",
        "\n",
        "    inputs: a (B, L) tensor of token indices\n",
        "    \"\"\"\n",
        "    B, L = inputs.size()\n",
        "\n",
        "    \"\"\"\n",
        "    Reshaping.\n",
        "    Takes in a (B, L) LongTensor\n",
        "    Outputs a (B, L~, S) LongTensor\n",
        "    \"\"\"\n",
        "    # Fist, get our word windows for each word in our input.\n",
        "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
        "    _, adjusted_length, _ = token_windows.size()\n",
        "\n",
        "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
        "    # 检查形状对不对\n",
        "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
        "\n",
        "    \"\"\"\n",
        "    Embedding.\n",
        "    Takes in a torch.LongTensor of size (B, L~, S)\n",
        "    Outputs a (B, L~, S, D) FloatTensor.\n",
        "    \"\"\"\n",
        "    embedded_windows = self.embeds(token_windows)\n",
        "\n",
        "    \"\"\"\n",
        "    Reshaping.\n",
        "    Takes in a (B, L~, S, D) FloatTensor.\n",
        "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
        "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
        "    \"\"\"\n",
        "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
        "\n",
        "    \"\"\"\n",
        "    Layer 1.\n",
        "    Takes in a (B, L~, S*D) FloatTensor.\n",
        "    Resizes it into a (B, L~, H) FloatTensor\n",
        "    \"\"\"\n",
        "    layer_1 = self.hidden_layer(embedded_windows)\n",
        "\n",
        "    \"\"\"\n",
        "    Layer 2\n",
        "    Takes in a (B, L~, H) FloatTensor.\n",
        "    Resizes it into a (B, L~, 1) FloatTensor.\n",
        "    \"\"\"\n",
        "    output = self.output_layer(layer_1)\n",
        "\n",
        "    \"\"\"\n",
        "    Softmax.\n",
        "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
        "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
        "    \"\"\"\n",
        "    output = self.probabilities(output)\n",
        "    output = output.view(B, -1)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avy1fnyAvEcd"
      },
      "source": [
        "### Training\n",
        "\n",
        "We are now ready to put everything together. Let's start with preparing our data and intializing our model. We can then intialize our optimizer and define our loss function. This time, instead of using one of the predefined loss function as we did before, we will define our own loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "bInu1VqjHsfj"
      },
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "data = list(zip(train_sentences, train_labels))\n",
        "batch_size = 2\n",
        "shuffle = True\n",
        "window_size = 2\n",
        "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
        "\n",
        "# Instantiate a DataLoader\n",
        "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize a model\n",
        "# It is useful to put all the model hyperparameters in a dictionary\n",
        "model_hyperparameters = {\n",
        "    \"batch_size\": 4,\n",
        "    \"window_size\": 2,\n",
        "    \"embed_dim\": 25,\n",
        "    \"hidden_dim\": 25,\n",
        "    \"freeze_embeddings\": False,\n",
        "}\n",
        "\n",
        "vocab_size = len(word_to_ix)\n",
        "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
        "\n",
        "# Define an optimizer\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define a loss function, which computes to binary cross entropy loss\n",
        "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
        "    # Calculate the loss for the whole batch\n",
        "    # BCELoss()常用于二分类任务的损失函数\n",
        "    bceloss = nn.BCELoss()\n",
        "    loss = bceloss(batch_outputs, batch_labels.float())\n",
        "\n",
        "    # Rescale the loss. Remember that we have used lengths to store the\n",
        "    # number of words in each training example\n",
        "    # 重新调整损失。请记住，我们使用长度来存储每个训练示例中的单词数量\n",
        "    loss = loss / batch_lengths.sum().float()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHxpxDkFHfQE"
      },
      "source": [
        "Unlike our earlier example, this time instead of passing all of our training data to the model at once in each epoch, we will be utilizing batches. Hence, in each training epoch iteration, we also iterate over the batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "QL9IDgIOvHca"
      },
      "outputs": [],
      "source": [
        "# Function that will be called in every epoch\n",
        "def train_epoch(loss_function, optimizer, model, loader):\n",
        "\n",
        "  # Keep track of the total loss for the batch\n",
        "  total_loss = 0\n",
        "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
        "    # Clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Run a forward pass\n",
        "    outputs = model.forward(batch_inputs)\n",
        "    # Compute the batch loss\n",
        "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
        "    # Calculate the gradients\n",
        "    loss.backward()\n",
        "    # Update the parameteres\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "# Function containing our main training loop\n",
        "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
        "\n",
        "  # Iterate through each epoch and call our train_epoch function\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
        "    if epoch % 100 == 0: print(epoch_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjf75cnzJ4n6"
      },
      "source": [
        "Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kav8kwVBJ6XW",
        "outputId": "0bafb3dc-6806-47bc-c5dc-915195e1d05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.24125424027442932\n",
            "0.22220438718795776\n",
            "0.17085469886660576\n",
            "0.11967884749174118\n",
            "0.10180189833045006\n",
            "0.0804689358919859\n",
            "0.057760629802942276\n",
            "0.051059434190392494\n",
            "0.03829230275005102\n",
            "0.04034239985048771\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 1000\n",
        "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-k7Pav4LdQJ"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "Let's see how well our model is at making predictions. We can start by creating our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "-v5X69a2Lkbm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['she', 'comes', 'from', 'paris'], [0, 0, 0, 1]), (['she', 'has', 'a', 'pretty', 'dog'], [0, 0, 0, 0, 0]), (['he', 'travels', 'to', 'china'], [0, 0, 0, 1])]\n"
          ]
        }
      ],
      "source": [
        "# Create test sentences\n",
        "test_corpus = [\"She comes from Paris\",\n",
        "               \"She has a pretty dog\",\n",
        "               \"He travels to China\"\n",
        "               ]\n",
        "test_sentences = [s.lower().split() for s in test_corpus]\n",
        "test_labels = [[0, 0, 0, 1],\n",
        "               [0, 0, 0, 0, 0],\n",
        "               [0, 0, 0, 1]]\n",
        "\n",
        "# Create a test loader\n",
        "test_data = list(zip(test_sentences, test_labels))\n",
        "batch_size = 1\n",
        "shuffle = False\n",
        "window_size = 2\n",
        "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                           batch_size=1,\n",
        "                                           shuffle=False,\n",
        "                                           collate_fn=collate_fn)\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlBa8xaNMZgv"
      },
      "source": [
        "Let's loop over our test examples to see how well we are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGYn8CAoMTjX",
        "outputId": "8d8fd126-ae7e-48e2-fa3e-8eb1a14f99c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 0, 1]])\n",
            "tensor([[0.1098, 0.0611, 0.0427, 0.8780]], grad_fn=<ViewBackward0>)\n",
            "tensor([[0, 0, 0, 0, 0]])\n",
            "tensor([[0.2230, 0.2685, 0.2744, 0.3572, 0.4100]], grad_fn=<ViewBackward0>)\n",
            "tensor([[0, 0, 0, 1]])\n",
            "tensor([[0.0521, 0.2016, 0.2370, 0.6765]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for test_instance, labels, _ in test_loader:\n",
        "  outputs = model.forward(test_instance)\n",
        "  print(labels)\n",
        "  print(outputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

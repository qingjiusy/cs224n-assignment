{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level.\n",
    "\n",
    "输入文件是简单的文本文件，我们将其分割为单个字符，然后在GPT上进行训练。所以你可以说这是一个字符transformer，而不是字符RNN。说起来不是很顺口。在这个例子中，我们会给它一些莎士比亚的文本，让它进行字符级别的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "# 这是一个标准库模块，用于记录日志信息\n",
    "# format：指定日志消息的格式。这里的格式字符串包含了几个占位符\n",
    "# datefmt：指定时间的显示格式，这里设置为 \"月/日/年 时:分:秒\"。\n",
    "# level：设置日志级别，这里设置为 logging.INFO，表示只记录 INFO 级别及以上的日志消息。\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "# 使程序具有确定性，确保每次运行时的结果是相同的\n",
    "# 设置随机种子\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        # 创建两个字典\n",
    "        # self.stoi（string to index）：将每个字符映射到一个唯一的索引。\n",
    "        # self.itos（index to string）：将每个索引映射回相应的字符。\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # vocab_size是chars的表的大小\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    # 用于获取指定索引处的数据样本\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # 取得一个block_size大小的窗口data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        # 每个字符转换为整数索引，结果存储在 dix 列表中\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "\n",
    "        将数据和目标排列，使得 x 的前 i 个元素将用于预测 y 的第 i 个元素。\n",
    "        注意，最终的语言模型实际上会基于这些数据同时进行 block_size 个单独的预测，\n",
    "        因此我们在前向传播过程中非常巧妙地分摊了网络的计算成本。\n",
    "        例如，如果 block_size 为 4, 那么我们可以采样一段文本 \"hello\",x 中的整数对应于 \"hell\",\n",
    "        y 中的整数对应于 \"ello\"。这实际上会在语言模型中同时 \"多任务\" 处理 4 个独立的例子：\n",
    "        - 仅给出 \"h\"，请预测下一个字符 \"e\"\n",
    "        - 给出 \"he\"，请预测下一个字符 \"l\"\n",
    "        - 给出 \"hel\"，请预测下一个字符 \"l\"\n",
    "        - 给出 \"hell\"，请预测下一个字符 \"o\"\n",
    "\n",
    "        此外，由于 DataLoader 会创建一批样本，因此在每次训练的前向/后向传播过程中，将同时训练大量的预测，\n",
    "        从而分摊了大量的计算成本。特别是，对于一个批量输入的整数 X (B, T)（其中 B 是批量大小,T 是 block_size)\n",
    "        和 Y (B, T)，网络在训练过程中将同时训练 B*T 个预测，一次性完成！当然，在测试时我们可以在批量 B 之间并行化，\n",
    "        但与训练不同，我们不能在时间维度 T 上并行化——我们必须运行一次前向传播以恢复每个批次维度的序列中的下一个字符，\n",
    "        并重复地总是输入下一个字符以获取下一个字符。\n",
    "\n",
    "        因此，是的，自回归模型在训练/测试时间上存在很大的不对称性。在训练期间，我们可以每次前向传播处理 B*T,\n",
    "        但在测试期间，我们只能每次前向传播处理 B, 而T次,需要 T 次前向传播。\n",
    "        \"\"\"\n",
    "        # dix[:-1] 表示从 dix 列表中取除了最后一个字符外的所有字符，并将其转换为 PyTorch 的长整型张量\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        # dix[1:] 表示从 dix 列表中取除了第一个字符外的所有字符\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128 # spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2024 22:48:26 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "# GPTConfig 类用于配置 GPT 模型的超参数\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "# TrainerConfig 用于设置训练的超参数\n",
    "# max_epochs=2：训练的最大轮数（epoch），即整个训练数据集将被完整训练的次数。\n",
    "# batch_size=512：每个训练批次的样本数量。\n",
    "# learning_rate=6e-4：学习率，控制模型更新权重的步长。\n",
    "# lr_decay=True：启用学习率衰减，随着训练的进行逐渐减小学习率。\n",
    "# warmup_tokens=512*20：用于设置学习率预热阶段的总标记数，通常在训练初期逐渐增加学习率。\n",
    "# final_tokens=2*len(train_dataset)*block_size：设置训练结束时的总标记数。\n",
    "# num_workers=4：用于加载数据时的并行工作线程数，帮助加快数据读取速度。\n",
    "tconf = TrainerConfig(max_epochs=2, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! that e'er this tongue of mine,\n",
      "That laid the sentence of dread banishment\n",
      "On yon proud man, should take it off again\n",
      "With words of sooth! O that I were as great\n",
      "As is my grief, or lesser than my name!\n",
      "Or that I could forget\n",
      "With Richmond, I'll tell you what I am,\n",
      "The Lord Aumerle, .\n",
      "\n",
      "CLAUDIO:\n",
      "The prenzie Angelo!\n",
      "\n",
      "ISABELLA:\n",
      "O, 'tis the cunning livery of hell,\n",
      "The damned'st body to invest and cover\n",
      "In prenzie guards! Dost thou think, Claudio?\n",
      "If I would yield him my virginity,\n",
      "Thou mightst be freed.\n",
      "\n",
      "CLAUDIO:\n",
      "O heavens! it cannot be.\n",
      "\n",
      "ISABELLA:\n",
      "Yes, he would give't thee, from this rank offence,\n",
      "So to offend him still. This night's the time\n",
      "That I should do what I abhor to name,\n",
      "Or else thou diest to-morrow.\n",
      "\n",
      "CLAUDIO:\n",
      "Thou shalt not do't.\n",
      "\n",
      "ISABELLA:\n",
      "O, were it but my life,\n",
      "I'ld throw it down for your deliverance\n",
      "As frankly as a pin.\n",
      "\n",
      "CLAUDIO:\n",
      "Thanks, dear Isabel.\n",
      "\n",
      "ISABELLA:\n",
      "Be ready, Claudio, for your death tomorrow.\n",
      "\n",
      "CLAUDIO:\n",
      "Yes. Has he affections\n",
      "That profit us.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "By the world they see the word in 's doom.\n",
      "\n",
      "ANGELO:\n",
      "Thou art the like, thus hate the course in heaven.\n",
      "What foul hath bled the wheel or at wild,\n",
      "And wash him fresh again with true-love tears.\n",
      "Ah, thou, the model where old Troy did stand,\n",
      "Thou map of honour, thou King Richard's tomb,\n",
      "And not King Richard; thou most beauteous inn,\n",
      "Why should hard-favour'd grief be lodged in thee,\n",
      "When triumph is become an alehouse guest?\n",
      "\n",
      "KING RICHARD II:\n",
      "Join not with grief, fair woman, do not so,\n",
      "To make my end too sudden: learn, good soul,\n",
      "To think our former state a happy dream;\n",
      "From which awaked, the truth of what we are\n",
      "Shows us but this: I am sworn brother, sweet,\n",
      "To grim Necessity, and he and I\n",
      "Will keep a league till death. Hie thee to France\n",
      "And cloister thee in some religious house:\n",
      "Our holy lives must win a new world's crown,\n",
      "Which our profane hours here have stricken down.\n",
      "\n",
      "QUEEN:\n",
      "What, is my Richard both in shape and mind\n",
      "Transform'd and weaken'd? hath Bolingbroke deposed\n",
      "Thine intellect? hath h\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "# 生成文本的起始输入\n",
    "context = \"O God, O God!\"\n",
    "\n",
    "# torch.tensor(...) 创建一个一维张量，包含上下文字符串 context 中每个字符的索引\n",
    "# [None, ...] 是 Python 中的切片语法，用于在张量的第0个维度上插入一个新的维度。\n",
    "# 这里的 None 表示在该位置插入一个新的维度，而 ... 表示保持后面的维度不变\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "\n",
    "# 调用 sample 函数进行文本生成。\n",
    "# model 是训练好的 GPT 模型。\n",
    "# x 是输入的上下文张量。\n",
    "# 2000 指定生成的字符数。\n",
    "# temperature=1.0 控制采样的随机性，值越高则生成的文本越随机，值越低则越保守。\n",
    "# sample=True 表示进行随机采样。\n",
    "# top_k=10 限制每次预测时考虑的候选字符数量，选择概率最高的 10 个字符进行采样。\n",
    "# [0] 获取返回值中的第一个元素，这里 y 是生成的字符索引\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
